{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "prefix = 'sagemaker/spark-preprocess-demo/' + timestamp_prefix\n",
    "input_prefix = prefix + '/input/raw/abalone'\n",
    "input_preprocessed_prefix = prefix + '/input/preprocessed/abalone'\n",
    "model_prefix = prefix + '/model'\n",
    "#Jay Change\n",
    "mleap_model_prefix = prefix + '/mleap-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-12 18:31:52--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.200.80\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.200.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 191873 (187K) [binary/octet-stream]\n",
      "Saving to: ‘abalone.csv’\n",
      "\n",
      "abalone.csv         100%[===================>] 187.38K   824KB/s    in 0.2s    \n",
      "\n",
      "2020-06-12 18:31:53 (824 KB/s) - ‘abalone.csv’ saved [191873/191873]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-328296961357/sagemaker/spark-preprocess-demo/2020-06-12-18-31-52/input/raw/abalone/abalone.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the dataset from the SageMaker bucket\n",
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv\n",
    "\n",
    "# Uploading the training data to S3\n",
    "sagemaker_session.upload_data(path='abalone.csv', bucket=bucket, key_prefix=input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/airflow-sagemaker/end-to-end-pipeline/container\n",
      "Sending build context to Docker daemon  17.36MB\n",
      "Step 1/34 : FROM openjdk:8-jre-slim\n",
      " ---> 778bc46b8d12\n",
      "Step 2/34 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 7098adcac4c9\n",
      "Step 3/34 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> f046d58588a5\n",
      "Step 4/34 : RUN pip3 install py4j psutil==5.6.5 mleap==0.8.1 boto3\n",
      " ---> Using cache\n",
      " ---> 663ef77ac837\n",
      "Step 5/34 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 2f65a6afff2e\n",
      "Step 6/34 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> f61c8c50496b\n",
      "Step 7/34 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> ba8a91d8e91b\n",
      "Step 8/34 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 1a871870b26b\n",
      "Step 9/34 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> b755367bdca0\n",
      "Step 10/34 : ENV HADOOP_VERSION 3.0.0\n",
      " ---> Using cache\n",
      " ---> 3cb2d40cbc79\n",
      "Step 11/34 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> bc56e7180eb1\n",
      "Step 12/34 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> fab377037386\n",
      "Step 13/34 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> e125b8fd8178\n",
      "Step 14/34 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Using cache\n",
      " ---> 1f228f7b3e9c\n",
      "Step 15/34 : ENV SPARK_VERSION 2.2.0\n",
      " ---> Using cache\n",
      " ---> 1adf5e122723\n",
      "Step 16/34 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Using cache\n",
      " ---> 6dc951631016\n",
      "Step 17/34 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Using cache\n",
      " ---> 5fec1cd55246\n",
      "Step 18/34 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*:$SPARK_HOME/input_custom_jars/*\"\n",
      " ---> Using cache\n",
      " ---> 99ecf2d0dc8a\n",
      "Step 19/34 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Using cache\n",
      " ---> 4b4d94641ee2\n",
      "Step 20/34 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> 4e10265b1fa5\n",
      "Step 21/34 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Using cache\n",
      " ---> 238e53568925\n",
      "Step 22/34 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> c393841b2126\n",
      "Step 23/34 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 770a8a8e959e\n",
      "Step 24/34 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 6721941b27fb\n",
      "Step 25/34 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 79432bb4638c\n",
      "Step 26/34 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> c4e8a9f3862b\n",
      "Step 27/34 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> c1ddafc0518d\n",
      "Step 28/34 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> 0a263fd3a1ef\n",
      "Step 29/34 : RUN chmod +x /opt/program/submit\n",
      " ---> Using cache\n",
      " ---> 1b26adf2a4cb\n",
      "Step 30/34 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> Using cache\n",
      " ---> 49d66706cc9a\n",
      "Step 31/34 : COPY mleap_spark_assembly.jar $SPARK_HOME/\n",
      " ---> Using cache\n",
      " ---> 8155b74eaf3c\n",
      "Step 32/34 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*:$SPARK_HOME/*\"\n",
      " ---> Using cache\n",
      " ---> e3c5e49f025a\n",
      "Step 33/34 : WORKDIR $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> fabdde4db3ce\n",
      "Step 34/34 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Using cache\n",
      " ---> 4969f61e5e12\n",
      "Successfully built 4969f61e5e12\n",
      "Successfully tagged sagemaker-spark-example:latest\n",
      "/home/ec2-user/SageMaker/airflow-sagemaker/end-to-end-pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd container\n",
    "!docker build -t sagemaker-spark-example .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'sagemaker-spark-example' already exists in the registry with id '328296961357'\n",
      "The push refers to repository [328296961357.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-example]\n",
      "\n",
      "\u001b[1Bedbf978f: Preparing \n",
      "\u001b[1Bd04b810b: Preparing \n",
      "\u001b[1Bc5c3c59f: Preparing \n",
      "\u001b[1B1506440b: Preparing \n",
      "\u001b[1Bb377677a: Preparing \n",
      "\u001b[1Bb24cd1fc: Preparing \n",
      "\u001b[1B9681573c: Preparing \n",
      "\u001b[1B82056577: Preparing \n",
      "\u001b[1Ba88de2cc: Preparing \n",
      "\u001b[1B87d8bf9e: Preparing \n",
      "\u001b[1B628f2670: Preparing \n",
      "\u001b[1B665fdbe9: Preparing \n",
      "\u001b[1Bcd2ee037: Preparing \n",
      "\u001b[1B386e5425: Preparing \n",
      "\u001b[1Bb21953f4: Layer already exists K\u001b[14A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[Klatest: digest: sha256:2c61b6939352ca9c924ebddeebea224f669afaa6bd2df480da1b2a8e69ced4c0 size: 3474\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "ecr_repository = 'sagemaker-spark-example'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "spark_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $spark_repository_uri\n",
    "!docker push $spark_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "import boto3\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "\n",
    "def csv_line(data):\n",
    "    r = ','.join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"PySparkAbalone\").getOrCreate()\n",
    "    \n",
    "    # Convert command line args into a map of args\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    args = dict(zip(args_iter, args_iter))\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\"mapred.output.committer.class\",\n",
    "                                                      \"org.apache.hadoop.mapred.FileOutputCommitter\")\n",
    "    \n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType([StructField(\"sex\", StringType(), True), \n",
    "                         StructField(\"length\", DoubleType(), True),\n",
    "                         StructField(\"diameter\", DoubleType(), True),\n",
    "                         StructField(\"height\", DoubleType(), True),\n",
    "                         StructField(\"whole_weight\", DoubleType(), True),\n",
    "                         StructField(\"shucked_weight\", DoubleType(), True),\n",
    "                         StructField(\"viscera_weight\", DoubleType(), True), \n",
    "                         StructField(\"shell_weight\", DoubleType(), True), \n",
    "                         StructField(\"rings\", DoubleType(), True)])\n",
    "\n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    total_df = spark.read.csv(('s3a://' + os.path.join(args['s3_input_bucket'], args['s3_input_key_prefix'],\n",
    "                                                   'abalone.csv')), header=False, schema=schema)\n",
    "\n",
    "    #StringIndexer on the sex column which has categorical value\n",
    "    sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"indexed_sex\")\n",
    "    \n",
    "    #one-hot-encoding is being performed on the string-indexed sex column (indexed_sex)\n",
    "    sex_encoder = OneHotEncoder(inputCol=\"indexed_sex\", outputCol=\"sex_vec\")\n",
    "\n",
    "    #vector-assembler will bring all the features to a 1D vector for us to save easily into CSV format\n",
    "    assembler = VectorAssembler(inputCols=[\"sex_vec\", \n",
    "                                           \"length\", \n",
    "                                           \"diameter\", \n",
    "                                           \"height\", \n",
    "                                           \"whole_weight\", \n",
    "                                           \"shucked_weight\", \n",
    "                                           \"viscera_weight\", \n",
    "                                           \"shell_weight\"], \n",
    "                                outputCol=\"features\")\n",
    "    \n",
    "    # The pipeline comprises of the steps added above\n",
    "    pipeline = Pipeline(stages=[sex_indexer, sex_encoder, assembler])\n",
    "    \n",
    "    # This step trains the feature transformers\n",
    "    model = pipeline.fit(total_df)\n",
    "    \n",
    "    # This step transforms the dataset with information obtained from the previous fit\n",
    "    transformed_total_df = model.transform(total_df)\n",
    "    \n",
    "    # Split the overall dataset into 80-20 training and validation\n",
    "    (train_df, validation_df) = transformed_total_df.randomSplit([0.8, 0.2])\n",
    "    \n",
    "    # Convert the train dataframe to RDD to save in CSV format and upload to S3\n",
    "    train_rdd = train_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    train_lines = train_rdd.map(csv_line)\n",
    "    train_lines.saveAsTextFile('s3a://' + os.path.join(args['s3_output_bucket'], args['s3_output_key_prefix'], 'train'))\n",
    "    \n",
    "    # Convert the validation dataframe to RDD to save in CSV format and upload to S3\n",
    "    validation_rdd = validation_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    validation_lines = validation_rdd.map(csv_line)\n",
    "    validation_lines.saveAsTextFile('s3a://' + os.path.join(args['s3_output_bucket'], args['s3_output_key_prefix'], 'validation'))\n",
    "    \n",
    "    # Serialize and store the model via MLeap  \n",
    "    SimpleSparkSerializer().serializeToBundle(model, \"jar:file:/opt/ml/model.zip\", validation_df)    \n",
    "    # Unzip the model as SageMaker expects a .tar.gz file but MLeap produces a .zip file\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"/opt/ml/model.zip\") as zf:\n",
    "        zf.extractall(\"/opt/ml/model\")\n",
    "\n",
    "    # Writw back the content as a .tar.gz file\n",
    "    import tarfile\n",
    "    with tarfile.open(\"/opt/ml/model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(\"/opt/ml/model/bundle.json\", arcname='bundle.json')\n",
    "        tar.add(\"/opt/ml/model/root\", arcname='root')\n",
    "    \n",
    "    # Upload the model in tar.gz format to S3 so that it can be used with SageMaker for inference later\n",
    "    s3 = boto3.resource('s3') \n",
    "    file_name = os.path.join(args['s3_mleap_model_prefix'], 'model.tar.gz')\n",
    "    s3.Bucket(args['s3_model_bucket']).upload_file('/opt/ml/model.tar.gz', file_name)    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-preprocessor-2020-06-12-18-31-56-434\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-328296961357/spark-preprocessor-2020-06-12-18-31-56-434/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n",
      "......................\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,248 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.71.156\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/s\u001b[0m\n",
      "\u001b[34mhare/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_252\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,254 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,258 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-91ec764c-f0c4-48f3-8f06-d805c86d8d4d\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,707 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,720 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,721 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,723 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,728 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,728 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,728 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,728 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,762 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,775 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,776 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,780 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,781 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Jun 12 18:35:27\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,782 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,782 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,783 INFO util.GSet: 2.0% max memory 6.7 GB = 136.4 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,783 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,838 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,842 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,843 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,875 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,875 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,875 INFO util.GSet: 1.0% max memory 6.7 GB = 68.2 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,875 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,893 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,893 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,893 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,893 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,898 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,902 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,902 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,902 INFO util.GSet: 0.25% max memory 6.7 GB = 17.0 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,902 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,909 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,909 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,909 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,913 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,913 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,914 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,914 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,915 INFO util.GSet: 0.029999999329447746% max memory 6.7 GB = 2.0 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,915 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,936 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1433587459-10.0.71.156-1591986927931\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,948 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:27,960 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:28,042 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:28,053 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:28,057 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.71.156\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.0.0/bin/../libexec/hadoop-functions.sh: line 981: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.0.0/bin/../libexec/hadoop-functions.sh: line 981: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-0-71-156.ec2.internal]\u001b[0m\n",
      "\u001b[34mip-10-0-71-156.ec2.internal: /usr/hadoop-3.0.0/bin/../libexec/hadoop-functions.sh: line 981: ssh: command not found\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.0.0/bin/../libexec/hadoop-functions.sh: line 981: ssh: command not found\u001b[0m\n",
      "\u001b[34mIvy Default Cache set to: /root/.ivy2/cache\u001b[0m\n",
      "\u001b[34mThe jars for the packages stored in: /root/.ivy2/jars\u001b[0m\n",
      "\u001b[34m:: loading settings :: url = jar:file:/usr/spark-2.2.0/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\u001b[0m\n",
      "\u001b[34mml.combust.mleap#mleap-spark_2.11 added as a dependency\u001b[0m\n",
      "\u001b[34m:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\u001b[0m\n",
      "\u001b[34m#011confs: [default]\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-spark_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-spark-base_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-runtime_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-core_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-base_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-tensor_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found io.spray#spray-json_2.11;1.3.2 in central\u001b[0m\n",
      "\u001b[34m#011found org.apache.spark#spark-mllib-local_2.11;2.2.0 in central\u001b[0m\n",
      "\u001b[34m#011found org.scalanlp#breeze_2.11;0.13.1 in central\u001b[0m\n",
      "\u001b[34m#011found org.scalanlp#breeze-macros_2.11;0.13.1 in central\u001b[0m\n",
      "\u001b[34m#011found org.scala-lang#scala-reflect;2.11.8 in central\u001b[0m\n",
      "\u001b[34m#011found com.github.fommil.netlib#core;1.1.2 in central\u001b[0m\n",
      "\u001b[34m#011found net.sourceforge.f2j#arpack_combined_all;0.1 in central\u001b[0m\n",
      "\u001b[34m#011found net.sf.opencsv#opencsv;2.3 in central\u001b[0m\n",
      "\u001b[34m#011found com.github.rwl#jtransforms;2.4.0 in central\u001b[0m\n",
      "\u001b[34m#011found junit#junit;4.12 in central\u001b[0m\n",
      "\u001b[34m#011found org.hamcrest#hamcrest-core;1.3 in central\u001b[0m\n",
      "\u001b[34m#011found org.spire-math#spire_2.11;0.13.0 in central\u001b[0m\n",
      "\u001b[34m#011found org.spire-math#spire-macros_2.11;0.13.0 in central\u001b[0m\n",
      "\u001b[34m#011found org.typelevel#machinist_2.11;0.6.1 in central\u001b[0m\n",
      "\u001b[34m#011found com.chuusai#shapeless_2.11;2.3.2 in central\u001b[0m\n",
      "\u001b[34m#011found org.typelevel#macro-compat_2.11;1.1.1 in central\u001b[0m\n",
      "\u001b[34m#011found org.slf4j#slf4j-api;1.7.16 in central\u001b[0m\n",
      "\u001b[34m#011found org.apache.commons#commons-math3;3.4.1 in central\u001b[0m\n",
      "\u001b[34m#011found org.apache.spark#spark-tags_2.11;2.2.0 in central\u001b[0m\n",
      "\u001b[34m#011found org.spark-project.spark#unused;1.0.0 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.bundle#bundle-ml_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found com.trueaccord.scalapb#scalapb-runtime_2.11;0.6.0 in central\u001b[0m\n",
      "\u001b[34m#011found com.trueaccord.lenses#lenses_2.11;0.4.12 in central\u001b[0m\n",
      "\u001b[34m#011found com.lihaoyi#fastparse_2.11;0.4.2 in central\u001b[0m\n",
      "\u001b[34m#011found com.lihaoyi#fastparse-utils_2.11;0.4.2 in central\u001b[0m\n",
      "\u001b[34m#011found com.lihaoyi#sourcecode_2.11;0.1.3 in central\u001b[0m\n",
      "\u001b[34m#011found com.google.protobuf#protobuf-java;3.3.1 in central\u001b[0m\n",
      "\u001b[34m#011found com.jsuereth#scala-arm_2.11;2.0 in central\u001b[0m\n",
      "\u001b[34m#011found com.typesafe#config;1.3.0 in central\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-spark_2.11/0.8.1/mleap-spark_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-spark_2.11;0.8.1!mleap-spark_2.11.jar (23ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-spark-base_2.11/0.8.1/mleap-spark-base_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-spark-base_2.11;0.8.1!mleap-spark-base_2.11.jar (8ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-runtime_2.11/0.8.1/mleap-runtime_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-runtime_2.11;0.8.1!mleap-runtime_2.11.jar (43ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-core_2.11/0.8.1/mleap-core_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-core_2.11;0.8.1!mleap-core_2.11.jar (23ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/bundle/bundle-ml_2.11/0.8.1/bundle-ml_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.bundle#bundle-ml_2.11;0.8.1!bundle-ml_2.11.jar (34ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.8/scala-reflect-2.11.8.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.8!scala-reflect.jar (97ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-base_2.11/0.8.1/mleap-base_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-base_2.11;0.8.1!mleap-base_2.11.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-tensor_2.11/0.8.1/mleap-tensor_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-tensor_2.11;0.8.1!mleap-tensor_2.11.jar (5ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.apache.spark#spark-mllib-local_2.11;2.2.0!spark-mllib-local_2.11.jar (9ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.github.rwl#jtransforms;2.4.0!jtransforms.jar (30ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/io/spray/spray-json_2.11/1.3.2/spray-json_2.11-1.3.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] io.spray#spray-json_2.11;1.3.2!spray-json_2.11.jar(bundle) (14ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.scalanlp#breeze_2.11;0.13.1!breeze_2.11.jar (167ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.apache.commons#commons-math3;3.4.1!commons-math3.jar (22ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.apache.spark#spark-tags_2.11;2.2.0!spark-tags_2.11.jar (3ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.scalanlp#breeze-macros_2.11;0.13.1!breeze-macros_2.11.jar (4ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (4ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1-javadoc.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] net.sourceforge.f2j#arpack_combined_all;0.1!arpack_combined_all.jar (49ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] net.sf.opencsv#opencsv;2.3!opencsv.jar (7ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.spire-math#spire_2.11;0.13.0!spire_2.11.jar (68ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.chuusai#shapeless_2.11;2.3.2!shapeless_2.11.jar(bundle) (25ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (5ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/junit/junit/4.12/junit-4.12.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] junit#junit;4.12!junit.jar (5ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.hamcrest#hamcrest-core;1.3!hamcrest-core.jar (3ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.spire-math#spire-macros_2.11;0.13.0!spire-macros_2.11.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.typelevel#machinist_2.11;0.6.1!machinist_2.11.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.typelevel#macro-compat_2.11;1.1.1!macro-compat_2.11.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/trueaccord/scalapb/scalapb-runtime_2.11/0.6.0/scalapb-runtime_2.11-0.6.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.trueaccord.scalapb#scalapb-runtime_2.11;0.6.0!scalapb-runtime_2.11.jar (29ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/jsuereth/scala-arm_2.11/2.0/scala-arm_2.11-2.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.jsuereth#scala-arm_2.11;2.0!scala-arm_2.11.jar (3ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/typesafe/config/1.3.0/config-1.3.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.typesafe#config;1.3.0!config.jar(bundle) (4ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/trueaccord/lenses/lenses_2.11/0.4.12/lenses_2.11-0.4.12.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.trueaccord.lenses#lenses_2.11;0.4.12!lenses_2.11.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/lihaoyi/fastparse_2.11/0.4.2/fastparse_2.11-0.4.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.lihaoyi#fastparse_2.11;0.4.2!fastparse_2.11.jar (5ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.3.1/protobuf-java-3.3.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.google.protobuf#protobuf-java;3.3.1!protobuf-java.jar(bundle) (11ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/lihaoyi/fastparse-utils_2.11/0.4.2/fastparse-utils_2.11-0.4.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.lihaoyi#fastparse-utils_2.11;0.4.2!fastparse-utils_2.11.jar (4ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.11/0.1.3/sourcecode_2.11-0.1.3.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.lihaoyi#sourcecode_2.11;0.1.3!sourcecode_2.11.jar (3ms)\u001b[0m\n",
      "\u001b[34m:: resolution report :: resolve 3089ms :: artifacts dl 736ms\u001b[0m\n",
      "\u001b[34m#011:: modules in use:\u001b[0m\n",
      "\u001b[34m#011com.chuusai#shapeless_2.11;2.3.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.github.fommil.netlib#core;1.1.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.github.rwl#jtransforms;2.4.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.google.protobuf#protobuf-java;3.3.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.jsuereth#scala-arm_2.11;2.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.lihaoyi#fastparse-utils_2.11;0.4.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.lihaoyi#fastparse_2.11;0.4.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.lihaoyi#sourcecode_2.11;0.1.3 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.trueaccord.lenses#lenses_2.11;0.4.12 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.trueaccord.scalapb#scalapb-runtime_2.11;0.6.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.typesafe#config;1.3.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011io.spray#spray-json_2.11;1.3.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011junit#junit;4.12 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.bundle#bundle-ml_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-base_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-core_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-runtime_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-spark-base_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-spark_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-tensor_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011net.sf.opencsv#opencsv;2.3 from central in [default]\u001b[0m\n",
      "\u001b[34m#011net.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.apache.commons#commons-math3;3.4.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.apache.spark#spark-mllib-local_2.11;2.2.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.apache.spark#spark-tags_2.11;2.2.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.hamcrest#hamcrest-core;1.3 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.scala-lang#scala-reflect;2.11.8 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.scalanlp#breeze-macros_2.11;0.13.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.scalanlp#breeze_2.11;0.13.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.slf4j#slf4j-api;1.7.16 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.spark-project.spark#unused;1.0.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.spire-math#spire-macros_2.11;0.13.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.spire-math#spire_2.11;0.13.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.typelevel#machinist_2.11;0.6.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.typelevel#macro-compat_2.11;1.1.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011---------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m#011|                  |            modules            ||   artifacts   |\u001b[0m\n",
      "\u001b[34m#011|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\u001b[0m\n",
      "\u001b[34m#011---------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m#011|      default     |   35  |   35  |   35  |   0   ||   35  |   35  |\u001b[0m\n",
      "\u001b[34m#011---------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m:: retrieving :: org.apache.spark#spark-submit-parent\u001b[0m\n",
      "\u001b[34m#011confs: [default]\u001b[0m\n",
      "\u001b[34m#01135 artifacts copied, 0 already retrieved (52376kB/61ms)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,258 INFO spark.SparkContext: Running Spark version 2.2.0\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,405 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,479 INFO spark.SparkContext: Submitted application: PySparkAbalone\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,492 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,492 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,492 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,493 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,493 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,674 INFO util.Utils: Successfully started service 'sparkDriver' on port 37359.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,689 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,702 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,704 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,705 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,711 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-f44e011b-f1fc-4718-98d4-6988e34d2352\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,729 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,806 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,865 INFO util.log: Logging initialized @6028ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,908 INFO server.Server: jetty-9.3.z-SNAPSHOT\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,920 INFO server.Server: Started @6085ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,934 INFO server.AbstractConnector: Started ServerConnector@15c0e802{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,935 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,955 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3937123{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,956 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2343e77d{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,956 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65cf1428{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,957 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31a084c1{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,958 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ef96441{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,958 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6adf42e{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,959 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5235224e{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,960 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6feaf8fd{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,960 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a99b136{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,961 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32e16216{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,961 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15156b0c{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,962 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4f4aa9e{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,962 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ba7ed96{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,963 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23b4a1eb{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,963 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b6bde1{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,964 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@294d9791{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,964 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31afa222{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,965 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5dc112d2{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,965 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30a690c0{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,966 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@529cb34a{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,972 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@eaa9101{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,972 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a9e1380{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,973 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6aa50152{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,974 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22708c80{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,974 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9aa5c39{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:45,976 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.71.156:4040\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:46,829 INFO client.RMProxy: Connecting to ResourceManager at /10.0.71.156:8032\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:47,057 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:47,123 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:47,123 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:47,130 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (31706 MB per container)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:47,130 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:47,131 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:47,133 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:47,139 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:47,987 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:48,631 INFO yarn.Client: Uploading resource file:/tmp/spark-8b4e7317-b448-4fa6-8531-2405641a3723/__spark_libs__1912608713194894962.zip -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/__spark_libs__1912608713194894962.zip\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,054 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-spark_2.11-0.8.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/ml.combust.mleap_mleap-spark_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,085 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-spark-base_2.11-0.8.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/ml.combust.mleap_mleap-spark-base_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,109 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-runtime_2.11-0.8.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/ml.combust.mleap_mleap-runtime_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,135 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-core_2.11-0.8.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/ml.combust.mleap_mleap-core_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,161 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.bundle_bundle-ml_2.11-0.8.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/ml.combust.bundle_bundle-ml_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,188 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.scala-lang_scala-reflect-2.11.8.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,226 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-base_2.11-0.8.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/ml.combust.mleap_mleap-base_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,250 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-tensor_2.11-0.8.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/ml.combust.mleap_mleap-tensor_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,271 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.apache.spark_spark-mllib-local_2.11-2.2.0.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.apache.spark_spark-mllib-local_2.11-2.2.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,291 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.github.rwl_jtransforms-2.4.0.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.github.rwl_jtransforms-2.4.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,316 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/io.spray_spray-json_2.11-1.3.2.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/io.spray_spray-json_2.11-1.3.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,345 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.scalanlp_breeze_2.11-0.13.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.scalanlp_breeze_2.11-0.13.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,408 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-math3-3.4.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.apache.commons_commons-math3-3.4.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,459 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.2.0.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.apache.spark_spark-tags_2.11-2.2.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,483 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.spark-project.spark_unused-1.0.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,506 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.scalanlp_breeze-macros_2.11-0.13.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.scalanlp_breeze-macros_2.11-0.13.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,528 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.github.fommil.netlib_core-1.1.2.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.github.fommil.netlib_core-1.1.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,950 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/net.sourceforge.f2j_arpack_combined_all-0.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/net.sourceforge.f2j_arpack_combined_all-0.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:50,984 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/net.sf.opencsv_opencsv-2.3.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/net.sf.opencsv_opencsv-2.3.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,003 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.spire-math_spire_2.11-0.13.0.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.spire-math_spire_2.11-0.13.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,047 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.chuusai_shapeless_2.11-2.3.2.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.chuusai_shapeless_2.11-2.3.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,080 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.slf4j_slf4j-api-1.7.16.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,103 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/junit_junit-4.12.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/junit_junit-4.12.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,134 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.hamcrest_hamcrest-core-1.3.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.hamcrest_hamcrest-core-1.3.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,152 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.spire-math_spire-macros_2.11-0.13.0.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.spire-math_spire-macros_2.11-0.13.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,169 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.typelevel_machinist_2.11-0.6.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.typelevel_machinist_2.11-0.6.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,185 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.typelevel_macro-compat_2.11-1.1.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/org.typelevel_macro-compat_2.11-1.1.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,202 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.trueaccord.scalapb_scalapb-runtime_2.11-0.6.0.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.trueaccord.scalapb_scalapb-runtime_2.11-0.6.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,233 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.jsuereth_scala-arm_2.11-2.0.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.jsuereth_scala-arm_2.11-2.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,251 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.typesafe_config-1.3.0.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.typesafe_config-1.3.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,272 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.trueaccord.lenses_lenses_2.11-0.4.12.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.trueaccord.lenses_lenses_2.11-0.4.12.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,289 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.lihaoyi_fastparse_2.11-0.4.2.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.lihaoyi_fastparse_2.11-0.4.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,305 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.google.protobuf_protobuf-java-3.3.1.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.google.protobuf_protobuf-java-3.3.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,326 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.lihaoyi_fastparse-utils_2.11-0.4.2.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.lihaoyi_fastparse-utils_2.11-0.4.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,345 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.lihaoyi_sourcecode_2.11-0.1.3.jar -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/com.lihaoyi_sourcecode_2.11-0.1.3.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,364 INFO yarn.Client: Uploading resource file:/usr/spark-2.2.0/python/lib/pyspark.zip -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,383 INFO yarn.Client: Uploading resource file:/usr/spark-2.2.0/python/lib/py4j-0.10.4-src.zip -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/py4j-0.10.4-src.zip\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-spark_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-spark-base_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-runtime_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-core_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.bundle_bundle-ml_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-base_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-tensor_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.spark_spark-mllib-local_2.11-2.2.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.github.rwl_jtransforms-2.4.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/io.spray_spray-json_2.11-1.3.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.scalanlp_breeze_2.11-0.13.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.commons_commons-math3-3.4.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.2.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.scalanlp_breeze-macros_2.11-0.13.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.github.fommil.netlib_core-1.1.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,401 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/net.sourceforge.f2j_arpack_combined_all-0.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/net.sf.opencsv_opencsv-2.3.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.spire-math_spire_2.11-0.13.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.chuusai_shapeless_2.11-2.3.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/junit_junit-4.12.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.hamcrest_hamcrest-core-1.3.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.spire-math_spire-macros_2.11-0.13.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.typelevel_machinist_2.11-0.6.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.typelevel_macro-compat_2.11-1.1.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.trueaccord.scalapb_scalapb-runtime_2.11-0.6.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.jsuereth_scala-arm_2.11-2.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe_config-1.3.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.trueaccord.lenses_lenses_2.11-0.4.12.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.lihaoyi_fastparse_2.11-0.4.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.google.protobuf_protobuf-java-3.3.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.lihaoyi_fastparse-utils_2.11-0.4.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,402 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.lihaoyi_sourcecode_2.11-0.1.3.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,521 INFO yarn.Client: Uploading resource file:/tmp/spark-8b4e7317-b448-4fa6-8531-2405641a3723/__spark_conf__889504863996076797.zip -> hdfs://10.0.71.156/user/root/.sparkStaging/application_1591986937479_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,553 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,553 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,553 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,553 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,553 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,558 INFO yarn.Client: Submitting application application_1591986937479_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,741 INFO impl.YarnClientImpl: Submitted application application_1591986937479_0001\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:51,743 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1591986937479_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:52,749 INFO yarn.Client: Application report for application_1591986937479_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:52,751 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Fri Jun 12 18:35:52 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1591986951649\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1591986937479_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-06-12 18:35:53,753 INFO yarn.Client: Application report for application_1591986937479_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:54,755 INFO yarn.Client: Application report for application_1591986937479_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:55,758 INFO yarn.Client: Application report for application_1591986937479_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:56,761 INFO yarn.Client: Application report for application_1591986937479_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:57,690 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:57,694 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1591986937479_0001), /proxy/application_1591986937479_0001\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:57,695 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:57,763 INFO yarn.Client: Application report for application_1591986937479_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,766 INFO yarn.Client: Application report for application_1591986937479_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,766 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.102.241\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: 0\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1591986951649\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1591986937479_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,767 INFO cluster.YarnClientSchedulerBackend: Application application_1591986937479_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,772 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40203.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,773 INFO netty.NettyBlockTransferService: Server created on 10.0.71.156:40203\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,774 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,775 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.71.156, 40203, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,777 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.71.156:40203 with 366.3 MB RAM, BlockManagerId(driver, 10.0.71.156, 40203, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,779 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.71.156, 40203, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,779 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.71.156, 40203, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:35:58,792 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fffeb18{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:00,835 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.102.241:57512) with ID 1\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:00,886 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:34565 with 11.9 GB RAM, BlockManagerId(1, algo-2, 34565, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:16,116 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:16,284 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.2.0/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:16,284 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.2.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:16,288 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e999e64{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:16,289 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28e4644f{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:16,289 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6de44829{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:16,289 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61be7f27{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:16,290 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65fef5e{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:16,610 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:17,457 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,071 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,073 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, sex#0)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,074 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string>\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,080 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,426 INFO codegen.CodeGenerator: Code generated in 147.803284 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,472 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 444.5 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,513 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.5 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,515 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.71.156:40203 (size: 41.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,518 INFO spark.SparkContext: Created broadcast 0 from rdd at StringIndexer.scala:111\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,539 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,665 INFO spark.SparkContext: Starting job: countByValue at StringIndexer.scala:113\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,756 INFO scheduler.DAGScheduler: Registering RDD 6 (countByValue at StringIndexer.scala:113)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,758 INFO scheduler.DAGScheduler: Got job 0 (countByValue at StringIndexer.scala:113) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,758 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (countByValue at StringIndexer.scala:113)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,758 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,759 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,762 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at countByValue at StringIndexer.scala:113), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,820 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.5 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,822 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,822 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.71.156:40203 (size: 8.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,823 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,833 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at countByValue at StringIndexer.scala:113) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,834 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:18,853 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 5357 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:19,004 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:34565 (size: 8.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:19,682 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:34565 (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,177 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2330 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,179 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,184 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (countByValue at StringIndexer.scala:113) finished in 2.338 s\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,184 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,184 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,185 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,185 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,187 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[7] at countByValue at StringIndexer.scala:113), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,193 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,194 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1963.0 B, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,195 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.71.156:40203 (size: 1963.0 B, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,195 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,196 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[7] at countByValue at StringIndexer.scala:113) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,196 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,200 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-2, executor 1, partition 0, NODE_LOCAL, 4632 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,246 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:34565 (size: 1963.0 B, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,266 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.102.241:57512\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,268 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 136 bytes\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,316 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 118 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,317 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,318 INFO scheduler.DAGScheduler: ResultStage 1 (countByValue at StringIndexer.scala:113) finished in 0.119 s\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,321 INFO scheduler.DAGScheduler: Job 0 finished: countByValue at StringIndexer.scala:113, took 2.656078 s\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,536 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.71.156:40203 in memory (size: 41.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,537 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:34565 in memory (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,549 INFO spark.ContextCleaner: Cleaned shuffle 0\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,551 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.71.156:40203 in memory (size: 1963.0 B, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,554 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:34565 in memory (size: 1963.0 B, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,561 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.71.156:40203 in memory (size: 8.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,563 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:34565 in memory (size: 8.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,810 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,811 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,812 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string, length: double, diameter: double, height: double, whole_weight: double ... 7 more fields>\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,812 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,977 INFO codegen.CodeGenerator: Code generated in 127.4891 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:21,997 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 444.5 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,014 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 41.5 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,014 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.71.156:40203 (size: 41.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,015 INFO spark.SparkContext: Created broadcast 3 from javaToPython at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,016 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,193 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,194 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,800 INFO spark.SparkContext: Starting job: saveAsTextFile at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,800 INFO scheduler.DAGScheduler: Got job 1 (saveAsTextFile at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,801 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (saveAsTextFile at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,801 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,801 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,801 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,823 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.2 KB, free 365.7 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,824 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 64.1 KB, free 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,825 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.71.156:40203 (size: 64.1 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,825 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,826 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,826 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,826 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-2, executor 1, partition 0, PROCESS_LOCAL, 5368 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:22,838 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:34565 (size: 64.1 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:23,958 INFO spark.ContextCleaner: Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:23,958 INFO spark.ContextCleaner: Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:23,958 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:23,959 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:23,959 INFO spark.ContextCleaner: Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:23,959 INFO spark.ContextCleaner: Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:23,965 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:34565 (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-06-12 18:36:27,126 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 4300 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:27,127 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:27,128 INFO scheduler.DAGScheduler: ResultStage 2 (saveAsTextFile at NativeMethodAccessorImpl.java:0) finished in 4.302 s\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:27,129 INFO scheduler.DAGScheduler: Job 1 finished: saveAsTextFile at NativeMethodAccessorImpl.java:0, took 4.328914 s\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:27,987 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:27,987 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:27,988 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string, length: double, diameter: double, height: double, whole_weight: double ... 7 more fields>\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:27,988 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,078 INFO codegen.CodeGenerator: Code generated in 75.008771 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,097 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 444.5 KB, free 365.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,112 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 41.5 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,113 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.71.156:40203 (size: 41.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,114 INFO spark.SparkContext: Created broadcast 5 from javaToPython at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,114 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,272 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,272 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,661 INFO spark.SparkContext: Starting job: saveAsTextFile at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,661 INFO scheduler.DAGScheduler: Got job 2 (saveAsTextFile at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,661 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,662 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,662 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,662 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,680 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 174.2 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,682 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 64.0 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,683 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.71.156:40203 (size: 64.0 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,683 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,684 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,684 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,685 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 5368 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,696 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:34565 (size: 64.0 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:28,906 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:34565 (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:30,870 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2185 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:30,870 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:30,871 INFO scheduler.DAGScheduler: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:0) finished in 2.187 s\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:30,872 INFO scheduler.DAGScheduler: Job 2 finished: saveAsTextFile at NativeMethodAccessorImpl.java:0, took 2.210922 s\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,839 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,840 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,840 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string, length: double, diameter: double, height: double, whole_weight: double ... 7 more fields>\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,841 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,949 INFO codegen.CodeGenerator: Code generated in 61.655526 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,967 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 444.5 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,980 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 41.5 KB, free 364.4 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,980 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.71.156:40203 (size: 41.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,981 INFO spark.SparkContext: Created broadcast 7 from sparkToMleapDataShape at VectorAssemblerOp.scala:26\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,983 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,995 INFO spark.SparkContext: Starting job: sparkToMleapDataShape at VectorAssemblerOp.scala:26\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,995 INFO scheduler.DAGScheduler: Got job 3 (sparkToMleapDataShape at VectorAssemblerOp.scala:26) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,995 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (sparkToMleapDataShape at VectorAssemblerOp.scala:26)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,995 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,996 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:31,996 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[24] at sparkToMleapDataShape at VectorAssemblerOp.scala:26), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,000 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 56.4 KB, free 364.4 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,002 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.7 KB, free 364.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,002 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.71.156:40203 (size: 20.7 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,003 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,004 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[24] at sparkToMleapDataShape at VectorAssemblerOp.scala:26) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,004 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,005 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, algo-2, executor 1, partition 0, PROCESS_LOCAL, 5368 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,015 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:34565 (size: 20.7 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,086 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:34565 (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,233 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 228 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,233 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,234 INFO scheduler.DAGScheduler: ResultStage 4 (sparkToMleapDataShape at VectorAssemblerOp.scala:26) finished in 0.230 s\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,234 INFO scheduler.DAGScheduler: Job 3 finished: sparkToMleapDataShape at VectorAssemblerOp.scala:26, took 0.239272 s\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,253 INFO codegen.CodeGenerator: Code generated in 7.888802 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,501 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,504 INFO server.AbstractConnector: Stopped Spark@15c0e802{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,506 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.71.156:4040\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,510 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,519 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,520 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,523 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,525 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,537 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,543 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,543 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,543 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,545 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,547 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,547 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,548 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8b4e7317-b448-4fa6-8531-2405641a3723/pyspark-26dd6562-e6ae-464f-9869-aa6d4e7b3dc7\u001b[0m\n",
      "\u001b[34m2020-06-12 18:36:32,548 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-8b4e7317-b448-4fa6-8531-2405641a3723\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2020-06-12 18:36:34\u001b[0m\n",
      "\u001b[35mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mReceived end of job signal, exiting...\u001b[0m\n",
      "\u001b[34mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput\n",
    "spark_processor = ScriptProcessor(base_job_name='spark-preprocessor',\n",
    "                                  image_uri=spark_repository_uri,\n",
    "                                  command=['/opt/program/submit'],\n",
    "                                  role=role,\n",
    "                                  instance_count=2,\n",
    "                                  instance_type='ml.r5.xlarge',\n",
    "                                  max_runtime_in_seconds=1200,\n",
    "                                  env={'mode': 'python'})\n",
    "\n",
    "spark_processor.run(code='preprocess.py',\n",
    "                    arguments=['s3_input_bucket', bucket,\n",
    "                               's3_input_key_prefix', input_prefix,\n",
    "                               's3_output_bucket', bucket,\n",
    "                               's3_output_key_prefix', input_preprocessed_prefix,\n",
    "                               's3_model_bucket', bucket,\n",
    "                               's3_mleap_model_prefix', mleap_model_prefix],\n",
    "                    logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://sagemaker-us-east-1-328296961357/sagemaker/spark-preprocess-demo/2020-06-12-18-31-52/input/preprocessed/abalone/train/\n",
      "5.0,0.0,0.0,0.29,0.225,0.075,0.14,0.0515,0.0235,0.04\n",
      "7.0,0.0,0.0,0.305,0.225,0.07,0.1485,0.0585,0.0335,0.045\n",
      "7.0,0.0,0.0,0.325,0.26,0.09,0.1915,0.085,0.036,0.062\n",
      "9.0,0.0,0.0,0.33,0.26,0.08,0.2,0.0625,0.05,0.07\n",
      "6.0,0.0,0.0,0.335,0.22,0.07,0.17,0.076,0.0365,0.05\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 rows from s3://{}/{}/train/'.format(bucket, input_preprocessed_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$input_preprocessed_prefix/train/part-00000 - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sagemaker_session.boto_region_name, 'xgboost', repo_version=\"0.90-1\")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, input_preprocessed_prefix, 'train/part')\n",
    "s3_validation_data = 's3://{}/{}/{}'.format(bucket, input_preprocessed_prefix, 'validation/part')\n",
    "s3_output_location = 's3://{}/{}/{}'.format(bucket, prefix, 'xgboost_model')\n",
    "\n",
    "xgb_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                          role, \n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m4.xlarge',\n",
    "                                          train_volume_size = 20,\n",
    "                                          train_max_run = 3600,\n",
    "                                          input_mode= 'File',\n",
    "                                          output_path=s3_output_location,\n",
    "                                          sagemaker_session=sagemaker_session)\n",
    "\n",
    "xgb_model.set_hyperparameters(objective = \"reg:linear\",\n",
    "                              eta = .2,\n",
    "                              gamma = 4,\n",
    "                              max_depth = 5,\n",
    "                              num_round = 10,\n",
    "                              subsample = 0.7,\n",
    "                              silent = 0,\n",
    "                              min_child_weight = 6)\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/csv', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/csv', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 18:38:12 Starting - Starting the training job......\n",
      "2020-06-12 18:38:46 Starting - Launching requested ML instances......\n",
      "2020-06-12 18:40:07 Starting - Preparing the instances for training......\n",
      "2020-06-12 18:41:00 Downloading - Downloading input data...\n",
      "2020-06-12 18:41:31 Training - Downloading the training image..\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value reg:linear to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[18:41:53] 3353x9 matrix with 30177 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[18:41:53] 824x9 matrix with 7416 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 3353 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 824 rows\u001b[0m\n",
      "\u001b[34m[18:41:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:8.09075#011validation-rmse:8.17555\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:6.60923#011validation-rmse:6.70457\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:5.45201#011validation-rmse:5.55954\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:4.54846#011validation-rmse:4.67473\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:3.84052#011validation-rmse:3.97819\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:3.29537#011validation-rmse:3.46456\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:2.90293#011validation-rmse:3.0973\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:2.61272#011validation-rmse:2.82657\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:2.41146#011validation-rmse:2.64923\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:2.25515#011validation-rmse:2.51411\u001b[0m\n",
      "\n",
      "2020-06-12 18:42:04 Uploading - Uploading generated training model\n",
      "2020-06-12 18:42:04 Completed - Training job completed\n",
      "Training seconds: 64\n",
      "Billable seconds: 64\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-328296961357/sagemaker/spark-preprocess-demo/2020-06-12-18-31-52/xgboost_model/sagemaker-xgboost-2020-06-12-18-38-12-220/output/model.tar.gz'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_output_location + '/sagemaker-xgboost-2020-06-12-18-38-12-220/output/model.tar.gz'\n",
    "#s3://sagemaker-us-east-1-328296961357/sagemaker/spark-preprocess-demo/2020-06-12-18-31-52/xgboost_model/sagemaker-xgboost-2020-06-12-18-38-12-220/output/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"name\": \"sex\", \"type\": \"string\"}, {\"name\": \"length\", \"type\": \"double\"}, {\"name\": \"diameter\", \"type\": \"double\"}, {\"name\": \"height\", \"type\": \"double\"}, {\"name\": \"whole_weight\", \"type\": \"double\"}, {\"name\": \"shucked_weight\", \"type\": \"double\"}, {\"name\": \"viscera_weight\", \"type\": \"double\"}, {\"name\": \"shell_weight\", \"type\": \"double\"}], \"output\": {\"name\": \"features\", \"type\": \"double\", \"struct\": \"vector\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"sex\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"length\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"diameter\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"height\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"whole_weight\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"shucked_weight\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"viscera_weight\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"shell_weight\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"double\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "\n",
    "sparkml_data = 's3://{}/{}/{}'.format(bucket, mleap_model_prefix, 'model.tar.gz')\n",
    "# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\n",
    "xgb_model_data = s3_output_location + '/sagemaker-xgboost-2020-06-12-18-38-12-220/output/model.tar.gz'\n",
    "xgb_model = Model(model_data=xgb_model_data, image=training_image)\n",
    "#xgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: inference-pipeline-2020-06-12-18-31-52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'11.775216102600098'\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "payload = \"F,0.515,0.425,0.14,0.766,0.304,0.1725,0.255\"\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, serializer=csv_serializer,\n",
    "                                content_type=CONTENT_TYPE_CSV, accept=CONTENT_TYPE_CSV)\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'11.775216102600098'\n"
     ]
    }
   ],
   "source": [
    "payload = {\"data\": [\"F\",0.515,0.425,0.14,0.766,0.304,0.1725,0.255]}\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-12 18:50:58--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/batch_input_abalone.csv\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.242.64\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.242.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 654 [text/csv]\n",
      "Saving to: ‘batch_input_abalone.csv.1’\n",
      "\n",
      "batch_input_abalone 100%[===================>]     654  --.-KB/s    in 0s      \n",
      "\n",
      "2020-06-12 18:50:58 (16.7 MB/s) - ‘batch_input_abalone.csv.1’ saved [654/654]\n",
      "\n",
      "\n",
      "\n",
      "Showing first five lines\n",
      "\n",
      "M,0.455,0.365,0.095,0.514,0.2245,0.101,0.15\n",
      "M,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07\n",
      "F,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21\n",
      "M,0.44,0.365,0.125,0.516,0.2155,0.114,0.155\n",
      "I,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055\n",
      "\n",
      "\n",
      "As we can see, it is identical to the training file apart from the label being absent here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/batch_input_abalone.csv\n",
    "!printf \"\\n\\nShowing first five lines\\n\\n\"    \n",
    "!head -n 5 batch_input_abalone.csv \n",
    "!printf \"\\n\\nAs we can see, it is identical to the training file apart from the label being absent here.\\n\\n\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_loc = sagemaker_session.upload_data(path='batch_input_abalone.csv', bucket=bucket, key_prefix='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................\u001b[34m  .   ____          _            __ _ _\n",
      " /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\u001b[0m\n",
      "\u001b[34m( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n",
      " \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n",
      "  '  |____| .__|_| |_|_| |_\\__, | / / / /\n",
      " =========|_|==============|___/=/_/_/_/\n",
      " :: Spring Boot ::                  (v2.2)\n",
      "\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:49.105  INFO 8 --- [           main] com.amazonaws.sagemaker.App              : Starting App v2.2 on ef887a1a8771 with PID 8 (/usr/local/lib/sparkml-serving-2.2.jar started by root in /sagemaker-sparkml-model-server)\u001b[0m\n",
      "\u001b[35m  .   ____          _            __ _ _\n",
      " /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\u001b[0m\n",
      "\u001b[35m( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n",
      " \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n",
      "  '  |____| .__|_| |_|_| |_\\__, | / / / /\n",
      " =========|_|==============|___/=/_/_/_/\n",
      " :: Spring Boot ::                  (v2.2)\n",
      "\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:49.105  INFO 8 --- [           main] com.amazonaws.sagemaker.App              : Starting App v2.2 on ef887a1a8771 with PID 8 (/usr/local/lib/sparkml-serving-2.2.jar started by root in /sagemaker-sparkml-model-server)\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:49.110  INFO 8 --- [           main] com.amazonaws.sagemaker.App              : No active profile set, falling back to default profiles: default\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:52.569  INFO 8 --- [           main] org.eclipse.jetty.util.log               : Logging initialized @4695ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:49.110  INFO 8 --- [           main] com.amazonaws.sagemaker.App              : No active profile set, falling back to default profiles: default\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:52.569  INFO 8 --- [           main] org.eclipse.jetty.util.log               : Logging initialized @4695ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[32m[2020-06-12 18:54:49 +0000] [15] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[32m[2020-06-12 18:54:49 +0000] [15] [INFO] Listening at: unix:/tmp/gunicorn.sock (15)\u001b[0m\n",
      "\u001b[32m[2020-06-12 18:54:49 +0000] [15] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[32m[2020-06-12 18:54:49 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[32m[2020-06-12 18:54:49 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[32m[2020-06-12 18:54:49 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[32m[2020-06-12 18:54:49 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.000  INFO 8 --- [           main] o.s.b.w.e.j.JettyServletWebServerFactory : Server initialized with port: 8080\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.047  INFO 8 --- [           main] org.eclipse.jetty.server.Server          : jetty-9.4.z-SNAPSHOT; built: 2018-08-30T13:59:14.071Z; git: 27208684755d94a92186989f695db2d7b21ebc51; jvm 1.8.0_181-8u181-b13-2~deb9u1-b13\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.645  INFO 8 --- [           main] org.eclipse.jetty.server.session         : DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.646  INFO 8 --- [           main] org.eclipse.jetty.server.session         : No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.648  INFO 8 --- [           main] org.eclipse.jetty.server.session         : node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.683  INFO 8 --- [           main] o.e.j.s.h.ContextHandler.application     : Initializing Spring embedded WebApplicationContext\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.000  INFO 8 --- [           main] o.s.b.w.e.j.JettyServletWebServerFactory : Server initialized with port: 8080\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.047  INFO 8 --- [           main] org.eclipse.jetty.server.Server          : jetty-9.4.z-SNAPSHOT; built: 2018-08-30T13:59:14.071Z; git: 27208684755d94a92186989f695db2d7b21ebc51; jvm 1.8.0_181-8u181-b13-2~deb9u1-b13\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.645  INFO 8 --- [           main] org.eclipse.jetty.server.session         : DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.646  INFO 8 --- [           main] org.eclipse.jetty.server.session         : No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.648  INFO 8 --- [           main] org.eclipse.jetty.server.session         : node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.683  INFO 8 --- [           main] o.e.j.s.h.ContextHandler.application     : Initializing Spring embedded WebApplicationContext\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.685  INFO 8 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 4334 ms\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.685  INFO 8 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 4334 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.819  INFO 8 --- [           main] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.833  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.850  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.850  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'formContentFilter' to: [/*]\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.850  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.874  INFO 8 --- [           main] o.e.jetty.server.handler.ContextHandler  : Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@7fb9f71f{application,/,[file:///tmp/jetty-docbase.648627865476100294.8080/],AVAILABLE}\u001b[0m\n",
      "\u001b[34m2020-06-12 18:54:53.878  INFO 8 --- [           main] org.eclipse.jetty.server.Server          : Started @6009ms\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.819  INFO 8 --- [           main] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.833  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.850  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.850  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'formContentFilter' to: [/*]\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.850  INFO 8 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.874  INFO 8 --- [           main] o.e.jetty.server.handler.ContextHandler  : Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@7fb9f71f{application,/,[file:///tmp/jetty-docbase.648627865476100294.8080/],AVAILABLE}\u001b[0m\n",
      "\u001b[35m2020-06-12 18:54:53.878  INFO 8 --- [           main] org.eclipse.jetty.server.Server          : Started @6009ms\u001b[0m\n",
      "\n",
      "\u001b[32m[2020-06-12:18:55:21:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:21 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:21:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:21 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[18:55:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[18:55:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[36m2020-06-12T18:55:21.820:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=5, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 17 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[18:55:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 19 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[18:55:22] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 19 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 14 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[2020-06-12:18:55:22:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:18:55:22 +0000] \"POST /invocations HTTP/1.1\" 200 14 \"-\" \"Go-http-client/1.1\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data_path = 's3://{}/{}/{}'.format(bucket, 'batch', 'batch_input_abalone.csv')\n",
    "output_data_path = 's3://{}/{}/{}'.format(bucket, 'batch_output/abalone', timestamp_prefix)\n",
    "job_name = 'serial-inference-batch-' + timestamp_prefix\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    # This was the model created using PipelineModel and it contains feature processing and XGBoost\n",
    "    model_name = model_name,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='serial-inference-batch',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    accept = CONTENT_TYPE_CSV\n",
    ")\n",
    "transformer.transform(data = input_data_path,\n",
    "                      job_name = job_name,\n",
    "                      content_type = CONTENT_TYPE_CSV, \n",
    "                      split_type = 'Line')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (404) when calling the HeadObject operation: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b6e9aadce7dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mKEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'batch_output/abalone/2020-06-11-18-47-41/batch_input_abalone.csv.out'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_output_abalone.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head batch_output_abalone.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mbucket_download_file\u001b[0;34m(self, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    244\u001b[0m     return self.meta.client.download_file(\n\u001b[1;32m    245\u001b[0m         \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    170\u001b[0m         return transfer.download_file(\n\u001b[1;32m    171\u001b[0m             \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    305\u001b[0m             bucket, key, filename, extra_args, subscribers)\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;31m# This is for backwards compatibility where when retries are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# exceeded we need to throw the same error from boto3 instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Call the submit method to start submitting tasks to execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# transfer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# If there was an exception raised during the submission of task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/s3transfer/download.py\u001b[0m in \u001b[0;36m_submit\u001b[0;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m             transfer_future.meta.provide_transfer_size(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (404) when calling the HeadObject operation: Not Found"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "KEY = 'batch_output/abalone/2020-06-11-18-47-41/batch_input_abalone.csv.out'\n",
    "s3.Bucket(bucket).download_file(KEY, 'batch_output_abalone.csv')\n",
    "\n",
    "!head batch_output_abalone.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
