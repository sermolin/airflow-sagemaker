{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "prefix = 'sagemaker/spark-preprocess-demo/' + timestamp_prefix\n",
    "input_prefix = prefix + '/input/raw/abalone'\n",
    "input_preprocessed_prefix = prefix + '/input/preprocessed/abalone'\n",
    "model_prefix = prefix + '/model'\n",
    "#Jay Change\n",
    "mleap_model_prefix = prefix + '/mleap-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-12 19:58:14--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.220.56\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.220.56|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 191873 (187K) [binary/octet-stream]\n",
      "Saving to: ‘abalone.csv.1’\n",
      "\n",
      "abalone.csv.1       100%[===================>] 187.38K   772KB/s    in 0.2s    \n",
      "\n",
      "2020-06-12 19:58:15 (772 KB/s) - ‘abalone.csv.1’ saved [191873/191873]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-885332847160/sagemaker/spark-preprocess-demo/2020-06-12-19-58-14/input/raw/abalone/abalone.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch the dataset from the SageMaker bucket\n",
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/abalone/abalone.csv\n",
    "\n",
    "# Uploading the training data to S3\n",
    "sagemaker_session.upload_data(path='abalone.csv', bucket=bucket, key_prefix=input_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/airflow-sagemaker/end-to-end-pipeline/container\n",
      "Sending build context to Docker daemon  17.36MB\n",
      "Step 1/34 : FROM openjdk:8-jre-slim\n",
      " ---> 73c63778326a\n",
      "Step 2/34 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 4e81f4104846\n",
      "Step 3/34 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> cc99c960ee50\n",
      "Step 4/34 : RUN pip3 install py4j psutil==5.6.5 mleap==0.8.1 boto3\n",
      " ---> Using cache\n",
      " ---> a167dfbb669b\n",
      "Step 5/34 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 16778c248737\n",
      "Step 6/34 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 3554ce1d7583\n",
      "Step 7/34 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 3ff32b88f6ad\n",
      "Step 8/34 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> c4ba24c4a1fe\n",
      "Step 9/34 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> cd54353b9089\n",
      "Step 10/34 : ENV HADOOP_VERSION 3.0.0\n",
      " ---> Using cache\n",
      " ---> 83d8f51e985c\n",
      "Step 11/34 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> 570246bac697\n",
      "Step 12/34 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> 316be16bc089\n",
      "Step 13/34 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> d978cff6bccb\n",
      "Step 14/34 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Using cache\n",
      " ---> b75e909a8ae1\n",
      "Step 15/34 : ENV SPARK_VERSION 2.2.0\n",
      " ---> Using cache\n",
      " ---> 1eac2b197663\n",
      "Step 16/34 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Using cache\n",
      " ---> 1b1e589fd0f1\n",
      "Step 17/34 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Using cache\n",
      " ---> d9d55bb14f04\n",
      "Step 18/34 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*:$SPARK_HOME/input_custom_jars/*\"\n",
      " ---> Using cache\n",
      " ---> 7c4316e78cb7\n",
      "Step 19/34 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Using cache\n",
      " ---> 56a212552524\n",
      "Step 20/34 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> a62efe5da53e\n",
      "Step 21/34 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Using cache\n",
      " ---> ec1f85df97b0\n",
      "Step 22/34 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 7a68c054593f\n",
      "Step 23/34 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> c43f42e4b186\n",
      "Step 24/34 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 155d4dfbd610\n",
      "Step 25/34 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> abccc3fe54d7\n",
      "Step 26/34 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 1c8c90c1ce24\n",
      "Step 27/34 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 0e6d3db91e18\n",
      "Step 28/34 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> e94bb7d3ae4b\n",
      "Step 29/34 : RUN chmod +x /opt/program/submit\n",
      " ---> Using cache\n",
      " ---> 8c4a0e66e8b9\n",
      "Step 30/34 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> Using cache\n",
      " ---> 3e3475781b39\n",
      "Step 31/34 : COPY mleap_spark_assembly.jar $SPARK_HOME/\n",
      " ---> Using cache\n",
      " ---> 48f67ed5b4bf\n",
      "Step 32/34 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*:$SPARK_HOME/*\"\n",
      " ---> Using cache\n",
      " ---> 5119dca87838\n",
      "Step 33/34 : WORKDIR $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> cb068eec8e21\n",
      "Step 34/34 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Using cache\n",
      " ---> b28da33f6e1e\n",
      "Successfully built b28da33f6e1e\n",
      "Successfully tagged sagemaker-spark-example:latest\n",
      "/home/ec2-user/SageMaker/airflow-sagemaker/end-to-end-pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd container\n",
    "!docker build -t sagemaker-spark-example .\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'sagemaker-spark-example' already exists in the registry with id '885332847160'\n",
      "The push refers to repository [885332847160.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-example]\n",
      "\n",
      "\u001b[1Be3313c4d: Preparing \n",
      "\u001b[1B64df61d9: Preparing \n",
      "\u001b[1B79f1db19: Preparing \n",
      "\u001b[1B6a639273: Preparing \n",
      "\u001b[1Beba1bef5: Preparing \n",
      "\u001b[1Bfa011e85: Preparing \n",
      "\u001b[1Bbc7e7a20: Preparing \n",
      "\u001b[1B87ebe267: Preparing \n",
      "\u001b[1B98511571: Preparing \n",
      "\u001b[1Bdddeb7ee: Preparing \n",
      "\u001b[1B81dc20c2: Preparing \n",
      "\u001b[1B760baedf: Preparing \n",
      "\u001b[1B3663cf66: Preparing \n",
      "\u001b[1B29cec5e1: Preparing \n",
      "\u001b[2B29cec5e1: Layer already exists \u001b[11A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[Klatest: digest: sha256:2edc2d586f244e0211d305b1fc01ca257e113dc2a4d3c9dd4dc58469aaf45799 size: 3474\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "ecr_repository = 'sagemaker-spark-example'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "spark_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $spark_repository_uri\n",
    "!docker push $spark_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "import boto3\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import *\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "\n",
    "def csv_line(data):\n",
    "    r = ','.join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"PySparkAbalone\").getOrCreate()\n",
    "    \n",
    "    # Convert command line args into a map of args\n",
    "    args_iter = iter(sys.argv[1:])\n",
    "    args = dict(zip(args_iter, args_iter))\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\"mapred.output.committer.class\",\n",
    "                                                      \"org.apache.hadoop.mapred.FileOutputCommitter\")\n",
    "    \n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType([StructField(\"sex\", StringType(), True), \n",
    "                         StructField(\"length\", DoubleType(), True),\n",
    "                         StructField(\"diameter\", DoubleType(), True),\n",
    "                         StructField(\"height\", DoubleType(), True),\n",
    "                         StructField(\"whole_weight\", DoubleType(), True),\n",
    "                         StructField(\"shucked_weight\", DoubleType(), True),\n",
    "                         StructField(\"viscera_weight\", DoubleType(), True), \n",
    "                         StructField(\"shell_weight\", DoubleType(), True), \n",
    "                         StructField(\"rings\", DoubleType(), True)])\n",
    "\n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    total_df = spark.read.csv(('s3a://' + os.path.join(args['s3_input_bucket'], args['s3_input_key_prefix'],\n",
    "                                                   'abalone.csv')), header=False, schema=schema)\n",
    "\n",
    "    #StringIndexer on the sex column which has categorical value\n",
    "    sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"indexed_sex\")\n",
    "    \n",
    "    #one-hot-encoding is being performed on the string-indexed sex column (indexed_sex)\n",
    "    sex_encoder = OneHotEncoder(inputCol=\"indexed_sex\", outputCol=\"sex_vec\")\n",
    "\n",
    "    #vector-assembler will bring all the features to a 1D vector for us to save easily into CSV format\n",
    "    assembler = VectorAssembler(inputCols=[\"sex_vec\", \n",
    "                                           \"length\", \n",
    "                                           \"diameter\", \n",
    "                                           \"height\", \n",
    "                                           \"whole_weight\", \n",
    "                                           \"shucked_weight\", \n",
    "                                           \"viscera_weight\", \n",
    "                                           \"shell_weight\"], \n",
    "                                outputCol=\"features\")\n",
    "    \n",
    "    # The pipeline comprises of the steps added above\n",
    "    pipeline = Pipeline(stages=[sex_indexer, sex_encoder, assembler])\n",
    "    \n",
    "    # This step trains the feature transformers\n",
    "    model = pipeline.fit(total_df)\n",
    "    \n",
    "    # This step transforms the dataset with information obtained from the previous fit\n",
    "    transformed_total_df = model.transform(total_df)\n",
    "    \n",
    "    # Split the overall dataset into 80-20 training and validation\n",
    "    (train_df, validation_df) = transformed_total_df.randomSplit([0.8, 0.2])\n",
    "    \n",
    "    # Convert the train dataframe to RDD to save in CSV format and upload to S3\n",
    "    train_rdd = train_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    train_lines = train_rdd.map(csv_line)\n",
    "    train_lines.saveAsTextFile('s3a://' + os.path.join(args['s3_output_bucket'], args['s3_output_key_prefix'], 'train'))\n",
    "    \n",
    "    # Convert the validation dataframe to RDD to save in CSV format and upload to S3\n",
    "    validation_rdd = validation_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    validation_lines = validation_rdd.map(csv_line)\n",
    "    validation_lines.saveAsTextFile('s3a://' + os.path.join(args['s3_output_bucket'], args['s3_output_key_prefix'], 'validation'))\n",
    "    \n",
    "    # Serialize and store the model via MLeap  \n",
    "    SimpleSparkSerializer().serializeToBundle(model, \"jar:file:/opt/ml/model.zip\", validation_df)    \n",
    "    # Unzip the model as SageMaker expects a .tar.gz file but MLeap produces a .zip file\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(\"/opt/ml/model.zip\") as zf:\n",
    "        zf.extractall(\"/opt/ml/model\")\n",
    "\n",
    "    # Writw back the content as a .tar.gz file\n",
    "    import tarfile\n",
    "    with tarfile.open(\"/opt/ml/model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(\"/opt/ml/model/bundle.json\", arcname='bundle.json')\n",
    "        tar.add(\"/opt/ml/model/root\", arcname='root')\n",
    "    \n",
    "    # Upload the model in tar.gz format to S3 so that it can be used with SageMaker for inference later\n",
    "    s3 = boto3.resource('s3') \n",
    "    file_name = os.path.join(args['s3_mleap_model_prefix'], 'model.tar.gz')\n",
    "    s3.Bucket(args['s3_model_bucket']).upload_file('/opt/ml/model.tar.gz', file_name)    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-preprocessor-2020-06-12-19-58-18-035\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-885332847160/spark-preprocessor-2020-06-12-19-58-18-035/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n",
      ".........................\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:17,638 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.202.182\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-cli\u001b[0m\n",
      "\u001b[34ment-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_252\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:17,645 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:17,649 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-3aa79c09-dbfa-4730-ae31-3e521a3df128\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,112 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,123 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,124 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,126 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,130 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,130 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,130 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,131 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,163 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,177 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,177 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,181 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,181 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Jun 12 20:02:18\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,183 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,183 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,184 INFO util.GSet: 2.0% max memory 6.7 GB = 136.4 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,184 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,238 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,242 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,243 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,274 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,274 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,274 INFO util.GSet: 1.0% max memory 6.7 GB = 68.2 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,274 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,291 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,291 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,291 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,291 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,296 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,299 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,299 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,299 INFO util.GSet: 0.25% max memory 6.7 GB = 17.1 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,299 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,306 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,306 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,307 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,309 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,310 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,311 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,311 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,311 INFO util.GSet: 0.029999999329447746% max memory 6.7 GB = 2.0 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,311 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,332 INFO namenode.FSImage: Allocated new BlockPoolId: BP-743571917-10.0.202.182-1591992138326\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,343 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,352 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,431 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 386 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,442 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:18,445 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.202.182\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.0.0/bin/../libexec/hadoop-functions.sh: line 981: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.0.0/bin/../libexec/hadoop-functions.sh: line 981: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-0-202-182.ec2.internal]\u001b[0m\n",
      "\u001b[34mip-10-0-202-182.ec2.internal: /usr/hadoop-3.0.0/bin/../libexec/hadoop-functions.sh: line 981: ssh: command not found\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.0.0/bin/../libexec/hadoop-functions.sh: line 981: ssh: command not found\u001b[0m\n",
      "\u001b[34mIvy Default Cache set to: /root/.ivy2/cache\u001b[0m\n",
      "\u001b[34mThe jars for the packages stored in: /root/.ivy2/jars\u001b[0m\n",
      "\u001b[34m:: loading settings :: url = jar:file:/usr/spark-2.2.0/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\u001b[0m\n",
      "\u001b[34mml.combust.mleap#mleap-spark_2.11 added as a dependency\u001b[0m\n",
      "\u001b[34m:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\u001b[0m\n",
      "\u001b[34m#011confs: [default]\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-spark_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-spark-base_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-runtime_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-core_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-base_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.mleap#mleap-tensor_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found io.spray#spray-json_2.11;1.3.2 in central\u001b[0m\n",
      "\u001b[34m#011found org.apache.spark#spark-mllib-local_2.11;2.2.0 in central\u001b[0m\n",
      "\u001b[34m#011found org.scalanlp#breeze_2.11;0.13.1 in central\u001b[0m\n",
      "\u001b[34m#011found org.scalanlp#breeze-macros_2.11;0.13.1 in central\u001b[0m\n",
      "\u001b[34m#011found org.scala-lang#scala-reflect;2.11.8 in central\u001b[0m\n",
      "\u001b[34m#011found com.github.fommil.netlib#core;1.1.2 in central\u001b[0m\n",
      "\u001b[34m#011found net.sourceforge.f2j#arpack_combined_all;0.1 in central\u001b[0m\n",
      "\u001b[34m#011found net.sf.opencsv#opencsv;2.3 in central\u001b[0m\n",
      "\u001b[34m#011found com.github.rwl#jtransforms;2.4.0 in central\u001b[0m\n",
      "\u001b[34m#011found junit#junit;4.12 in central\u001b[0m\n",
      "\u001b[34m#011found org.hamcrest#hamcrest-core;1.3 in central\u001b[0m\n",
      "\u001b[34m#011found org.spire-math#spire_2.11;0.13.0 in central\u001b[0m\n",
      "\u001b[34m#011found org.spire-math#spire-macros_2.11;0.13.0 in central\u001b[0m\n",
      "\u001b[34m#011found org.typelevel#machinist_2.11;0.6.1 in central\u001b[0m\n",
      "\u001b[34m#011found com.chuusai#shapeless_2.11;2.3.2 in central\u001b[0m\n",
      "\u001b[34m#011found org.typelevel#macro-compat_2.11;1.1.1 in central\u001b[0m\n",
      "\u001b[34m#011found org.slf4j#slf4j-api;1.7.16 in central\u001b[0m\n",
      "\u001b[34m#011found org.apache.commons#commons-math3;3.4.1 in central\u001b[0m\n",
      "\u001b[34m#011found org.apache.spark#spark-tags_2.11;2.2.0 in central\u001b[0m\n",
      "\u001b[34m#011found org.spark-project.spark#unused;1.0.0 in central\u001b[0m\n",
      "\u001b[34m#011found ml.combust.bundle#bundle-ml_2.11;0.8.1 in central\u001b[0m\n",
      "\u001b[34m#011found com.trueaccord.scalapb#scalapb-runtime_2.11;0.6.0 in central\u001b[0m\n",
      "\u001b[34m#011found com.trueaccord.lenses#lenses_2.11;0.4.12 in central\u001b[0m\n",
      "\u001b[34m#011found com.lihaoyi#fastparse_2.11;0.4.2 in central\u001b[0m\n",
      "\u001b[34m#011found com.lihaoyi#fastparse-utils_2.11;0.4.2 in central\u001b[0m\n",
      "\u001b[34m#011found com.lihaoyi#sourcecode_2.11;0.1.3 in central\u001b[0m\n",
      "\u001b[34m#011found com.google.protobuf#protobuf-java;3.3.1 in central\u001b[0m\n",
      "\u001b[34m#011found com.jsuereth#scala-arm_2.11;2.0 in central\u001b[0m\n",
      "\u001b[34m#011found com.typesafe#config;1.3.0 in central\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-spark_2.11/0.8.1/mleap-spark_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-spark_2.11;0.8.1!mleap-spark_2.11.jar (19ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-spark-base_2.11/0.8.1/mleap-spark-base_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-spark-base_2.11;0.8.1!mleap-spark-base_2.11.jar (7ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-runtime_2.11/0.8.1/mleap-runtime_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-runtime_2.11;0.8.1!mleap-runtime_2.11.jar (30ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-core_2.11/0.8.1/mleap-core_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-core_2.11;0.8.1!mleap-core_2.11.jar (34ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/bundle/bundle-ml_2.11/0.8.1/bundle-ml_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.bundle#bundle-ml_2.11;0.8.1!bundle-ml_2.11.jar (33ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.8/scala-reflect-2.11.8.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.8!scala-reflect.jar (97ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-base_2.11/0.8.1/mleap-base_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-base_2.11;0.8.1!mleap-base_2.11.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/ml/combust/mleap/mleap-tensor_2.11/0.8.1/mleap-tensor_2.11-0.8.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] ml.combust.mleap#mleap-tensor_2.11;0.8.1!mleap-tensor_2.11.jar (3ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/apache/spark/spark-mllib-local_2.11/2.2.0/spark-mllib-local_2.11-2.2.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.apache.spark#spark-mllib-local_2.11;2.2.0!spark-mllib-local_2.11.jar (7ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.github.rwl#jtransforms;2.4.0!jtransforms.jar (21ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/io/spray/spray-json_2.11/1.3.2/spray-json_2.11-1.3.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] io.spray#spray-json_2.11;1.3.2!spray-json_2.11.jar(bundle) (9ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/scalanlp/breeze_2.11/0.13.1/breeze_2.11-0.13.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.scalanlp#breeze_2.11;0.13.1!breeze_2.11.jar (121ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.apache.commons#commons-math3;3.4.1!commons-math3.jar (16ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/apache/spark/spark-tags_2.11/2.2.0/spark-tags_2.11-2.2.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.apache.spark#spark-tags_2.11;2.2.0!spark-tags_2.11.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (4ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/scalanlp/breeze-macros_2.11/0.13.1/breeze-macros_2.11-0.13.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.scalanlp#breeze-macros_2.11;0.13.1!breeze-macros_2.11.jar (3ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (4ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1-javadoc.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] net.sourceforge.f2j#arpack_combined_all;0.1!arpack_combined_all.jar (46ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] net.sf.opencsv#opencsv;2.3!opencsv.jar (4ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/spire-math/spire_2.11/0.13.0/spire_2.11-0.13.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.spire-math#spire_2.11;0.13.0!spire_2.11.jar (65ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/chuusai/shapeless_2.11/2.3.2/shapeless_2.11-2.3.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.chuusai#shapeless_2.11;2.3.2!shapeless_2.11.jar(bundle) (23ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.16/slf4j-api-1.7.16.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.16!slf4j-api.jar (3ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/junit/junit/4.12/junit-4.12.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] junit#junit;4.12!junit.jar (4ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.hamcrest#hamcrest-core;1.3!hamcrest-core.jar (3ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/spire-math/spire-macros_2.11/0.13.0/spire-macros_2.11-0.13.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.spire-math#spire-macros_2.11;0.13.0!spire-macros_2.11.jar (3ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/typelevel/machinist_2.11/0.6.1/machinist_2.11-0.6.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.typelevel#machinist_2.11;0.6.1!machinist_2.11.jar (1ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/org/typelevel/macro-compat_2.11/1.1.1/macro-compat_2.11-1.1.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] org.typelevel#macro-compat_2.11;1.1.1!macro-compat_2.11.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/trueaccord/scalapb/scalapb-runtime_2.11/0.6.0/scalapb-runtime_2.11-0.6.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.trueaccord.scalapb#scalapb-runtime_2.11;0.6.0!scalapb-runtime_2.11.jar (31ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/jsuereth/scala-arm_2.11/2.0/scala-arm_2.11-2.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.jsuereth#scala-arm_2.11;2.0!scala-arm_2.11.jar (3ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/typesafe/config/1.3.0/config-1.3.0.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.typesafe#config;1.3.0!config.jar(bundle) (6ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/trueaccord/lenses/lenses_2.11/0.4.12/lenses_2.11-0.4.12.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.trueaccord.lenses#lenses_2.11;0.4.12!lenses_2.11.jar (2ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/lihaoyi/fastparse_2.11/0.4.2/fastparse_2.11-0.4.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.lihaoyi#fastparse_2.11;0.4.2!fastparse_2.11.jar (5ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.3.1/protobuf-java-3.3.1.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.google.protobuf#protobuf-java;3.3.1!protobuf-java.jar(bundle) (12ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/lihaoyi/fastparse-utils_2.11/0.4.2/fastparse-utils_2.11-0.4.2.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.lihaoyi#fastparse-utils_2.11;0.4.2!fastparse-utils_2.11.jar (6ms)\u001b[0m\n",
      "\u001b[34mdownloading https://repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.11/0.1.3/sourcecode_2.11-0.1.3.jar ...\u001b[0m\n",
      "\u001b[34m#011[SUCCESSFUL ] com.lihaoyi#sourcecode_2.11;0.1.3!sourcecode_2.11.jar (4ms)\u001b[0m\n",
      "\u001b[34m:: resolution report :: resolve 2967ms :: artifacts dl 648ms\u001b[0m\n",
      "\u001b[34m#011:: modules in use:\u001b[0m\n",
      "\u001b[34m#011com.chuusai#shapeless_2.11;2.3.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.github.fommil.netlib#core;1.1.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.github.rwl#jtransforms;2.4.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.google.protobuf#protobuf-java;3.3.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.jsuereth#scala-arm_2.11;2.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.lihaoyi#fastparse-utils_2.11;0.4.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.lihaoyi#fastparse_2.11;0.4.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.lihaoyi#sourcecode_2.11;0.1.3 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.trueaccord.lenses#lenses_2.11;0.4.12 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.trueaccord.scalapb#scalapb-runtime_2.11;0.6.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011com.typesafe#config;1.3.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011io.spray#spray-json_2.11;1.3.2 from central in [default]\u001b[0m\n",
      "\u001b[34m#011junit#junit;4.12 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.bundle#bundle-ml_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-base_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-core_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-runtime_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-spark-base_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-spark_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011ml.combust.mleap#mleap-tensor_2.11;0.8.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011net.sf.opencsv#opencsv;2.3 from central in [default]\u001b[0m\n",
      "\u001b[34m#011net.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.apache.commons#commons-math3;3.4.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.apache.spark#spark-mllib-local_2.11;2.2.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.apache.spark#spark-tags_2.11;2.2.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.hamcrest#hamcrest-core;1.3 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.scala-lang#scala-reflect;2.11.8 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.scalanlp#breeze-macros_2.11;0.13.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.scalanlp#breeze_2.11;0.13.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.slf4j#slf4j-api;1.7.16 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.spark-project.spark#unused;1.0.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.spire-math#spire-macros_2.11;0.13.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.spire-math#spire_2.11;0.13.0 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.typelevel#machinist_2.11;0.6.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011org.typelevel#macro-compat_2.11;1.1.1 from central in [default]\u001b[0m\n",
      "\u001b[34m#011---------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m#011|                  |            modules            ||   artifacts   |\u001b[0m\n",
      "\u001b[34m#011|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\u001b[0m\n",
      "\u001b[34m#011---------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m#011|      default     |   35  |   35  |   35  |   0   ||   35  |   35  |\u001b[0m\n",
      "\u001b[34m#011---------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m:: retrieving :: org.apache.spark#spark-submit-parent\u001b[0m\n",
      "\u001b[34m#011confs: [default]\u001b[0m\n",
      "\u001b[34m#01135 artifacts copied, 0 already retrieved (52376kB/59ms)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,143 INFO spark.SparkContext: Running Spark version 2.2.0\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,292 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,364 INFO spark.SparkContext: Submitted application: PySparkAbalone\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,378 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,378 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,379 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,379 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,379 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,582 INFO util.Utils: Successfully started service 'sparkDriver' on port 39807.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,604 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,621 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,623 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,623 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,630 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-da232889-267e-4cde-9293-b112b6ebe43e\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,646 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,719 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,785 INFO util.log: Logging initialized @5775ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,828 INFO server.Server: jetty-9.3.z-SNAPSHOT\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,839 INFO server.Server: Started @5830ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,855 INFO server.AbstractConnector: Started ServerConnector@70ed614f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,855 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,879 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8d4faf3{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,880 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@546234a0{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,880 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1742b39{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,881 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@361d701e{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,882 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d457f2b{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,882 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1978932d{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,883 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c61cade{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,884 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3eebf0f{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,884 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e55f3eb{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,885 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74c426bf{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,885 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@164955e6{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,886 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66d58c6c{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,886 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cc9b9fb{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,887 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@585113f9{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,887 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@500f3db6{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,888 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@781d7892{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,889 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@222dc583{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,889 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@212dc5f{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,890 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7059b48e{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,890 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@324bc53a{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,897 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@107afeb3{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,898 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7edc161d{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,899 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31217358{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,899 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@381169fa{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,900 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@aed9471{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:35,902 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.202.182:4040\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:36,787 INFO client.RMProxy: Connecting to ResourceManager at /10.0.202.182:8032\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:36,998 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:37,057 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:37,057 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:37,064 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (31706 MB per container)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:37,065 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:37,065 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:37,069 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:37,076 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:37,961 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:38,597 INFO yarn.Client: Uploading resource file:/tmp/spark-3661bfcc-5d8c-4484-921c-8b7b9142af23/__spark_libs__4211265429065248998.zip -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/__spark_libs__4211265429065248998.zip\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,678 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-spark_2.11-0.8.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/ml.combust.mleap_mleap-spark_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,706 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-spark-base_2.11-0.8.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/ml.combust.mleap_mleap-spark-base_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,735 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-runtime_2.11-0.8.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/ml.combust.mleap_mleap-runtime_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,759 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-core_2.11-0.8.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/ml.combust.mleap_mleap-core_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,779 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.bundle_bundle-ml_2.11-0.8.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/ml.combust.bundle_bundle-ml_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,805 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.scala-lang_scala-reflect-2.11.8.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,839 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-base_2.11-0.8.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/ml.combust.mleap_mleap-base_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,857 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-tensor_2.11-0.8.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/ml.combust.mleap_mleap-tensor_2.11-0.8.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,876 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.apache.spark_spark-mllib-local_2.11-2.2.0.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.apache.spark_spark-mllib-local_2.11-2.2.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,898 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.github.rwl_jtransforms-2.4.0.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.github.rwl_jtransforms-2.4.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,925 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/io.spray_spray-json_2.11-1.3.2.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/io.spray_spray-json_2.11-1.3.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:39,948 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.scalanlp_breeze_2.11-0.13.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.scalanlp_breeze_2.11-0.13.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,015 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-math3-3.4.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.apache.commons_commons-math3-3.4.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,043 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.2.0.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.apache.spark_spark-tags_2.11-2.2.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,064 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.spark-project.spark_unused-1.0.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,087 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.scalanlp_breeze-macros_2.11-0.13.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.scalanlp_breeze-macros_2.11-0.13.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,109 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.github.fommil.netlib_core-1.1.2.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.github.fommil.netlib_core-1.1.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,131 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/net.sourceforge.f2j_arpack_combined_all-0.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/net.sourceforge.f2j_arpack_combined_all-0.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,166 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/net.sf.opencsv_opencsv-2.3.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/net.sf.opencsv_opencsv-2.3.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,184 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.spire-math_spire_2.11-0.13.0.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.spire-math_spire_2.11-0.13.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,231 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.chuusai_shapeless_2.11-2.3.2.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.chuusai_shapeless_2.11-2.3.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,266 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.slf4j_slf4j-api-1.7.16.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,376 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/junit_junit-4.12.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/junit_junit-4.12.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,397 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.hamcrest_hamcrest-core-1.3.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.hamcrest_hamcrest-core-1.3.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,431 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.spire-math_spire-macros_2.11-0.13.0.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.spire-math_spire-macros_2.11-0.13.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,848 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.typelevel_machinist_2.11-0.6.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.typelevel_machinist_2.11-0.6.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,866 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/org.typelevel_macro-compat_2.11-1.1.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/org.typelevel_macro-compat_2.11-1.1.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,883 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.trueaccord.scalapb_scalapb-runtime_2.11-0.6.0.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.trueaccord.scalapb_scalapb-runtime_2.11-0.6.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,929 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.jsuereth_scala-arm_2.11-2.0.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.jsuereth_scala-arm_2.11-2.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,948 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.typesafe_config-1.3.0.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.typesafe_config-1.3.0.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,965 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.trueaccord.lenses_lenses_2.11-0.4.12.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.trueaccord.lenses_lenses_2.11-0.4.12.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,982 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.lihaoyi_fastparse_2.11-0.4.2.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.lihaoyi_fastparse_2.11-0.4.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:40,997 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.google.protobuf_protobuf-java-3.3.1.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.google.protobuf_protobuf-java-3.3.1.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,019 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.lihaoyi_fastparse-utils_2.11-0.4.2.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.lihaoyi_fastparse-utils_2.11-0.4.2.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,033 INFO yarn.Client: Uploading resource file:/root/.ivy2/jars/com.lihaoyi_sourcecode_2.11-0.1.3.jar -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/com.lihaoyi_sourcecode_2.11-0.1.3.jar\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,052 INFO yarn.Client: Uploading resource file:/usr/spark-2.2.0/python/lib/pyspark.zip -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,072 INFO yarn.Client: Uploading resource file:/usr/spark-2.2.0/python/lib/py4j-0.10.4-src.zip -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/py4j-0.10.4-src.zip\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-spark_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-spark-base_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-runtime_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-core_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.bundle_bundle-ml_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-base_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/ml.combust.mleap_mleap-tensor_2.11-0.8.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.spark_spark-mllib-local_2.11-2.2.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.github.rwl_jtransforms-2.4.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,491 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/io.spray_spray-json_2.11-1.3.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.scalanlp_breeze_2.11-0.13.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.commons_commons-math3-3.4.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.apache.spark_spark-tags_2.11-2.2.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.scalanlp_breeze-macros_2.11-0.13.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.github.fommil.netlib_core-1.1.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/net.sourceforge.f2j_arpack_combined_all-0.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/net.sf.opencsv_opencsv-2.3.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.spire-math_spire_2.11-0.13.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.chuusai_shapeless_2.11-2.3.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/junit_junit-4.12.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.hamcrest_hamcrest-core-1.3.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.spire-math_spire-macros_2.11-0.13.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.typelevel_machinist_2.11-0.6.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/org.typelevel_macro-compat_2.11-1.1.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.trueaccord.scalapb_scalapb-runtime_2.11-0.6.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.jsuereth_scala-arm_2.11-2.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.typesafe_config-1.3.0.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.trueaccord.lenses_lenses_2.11-0.4.12.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.lihaoyi_fastparse_2.11-0.4.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.google.protobuf_protobuf-java-3.3.1.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.lihaoyi_fastparse-utils_2.11-0.4.2.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,492 WARN yarn.Client: Same path resource file:/root/.ivy2/jars/com.lihaoyi_sourcecode_2.11-0.1.3.jar added multiple times to distributed cache.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,505 INFO yarn.Client: Uploading resource file:/tmp/spark-3661bfcc-5d8c-4484-921c-8b7b9142af23/__spark_conf__3947377133997921207.zip -> hdfs://10.0.202.182/user/root/.sparkStaging/application_1591992147615_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,537 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,537 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,537 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,537 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,537 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,542 INFO yarn.Client: Submitting application application_1591992147615_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,721 INFO impl.YarnClientImpl: Submitted application application_1591992147615_0001\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:41,723 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1591992147615_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:42,729 INFO yarn.Client: Application report for application_1591992147615_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:42,731 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Fri Jun 12 20:02:42 +0000 2020] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1591992161634\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1591992147615_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:43,733 INFO yarn.Client: Application report for application_1591992147615_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:44,736 INFO yarn.Client: Application report for application_1591992147615_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:45,738 INFO yarn.Client: Application report for application_1591992147615_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:46,742 INFO yarn.Client: Application report for application_1591992147615_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,361 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,364 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1591992147615_0001), /proxy/application_1591992147615_0001\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,366 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,744 INFO yarn.Client: Application report for application_1591992147615_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,744 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.230.110\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: 0\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1591992161634\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1591992147615_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,745 INFO cluster.YarnClientSchedulerBackend: Application application_1591992147615_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,772 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43709.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,772 INFO netty.NettyBlockTransferService: Server created on 10.0.202.182:43709\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,773 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,774 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.202.182, 43709, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,777 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.202.182:43709 with 366.3 MB RAM, BlockManagerId(driver, 10.0.202.182, 43709, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,779 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.202.182, 43709, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,780 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.202.182, 43709, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:47,790 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68af20da{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:51,636 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.230.110:53098) with ID 1\u001b[0m\n",
      "\u001b[34m2020-06-12 20:02:51,669 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:34019 with 11.9 GB RAM, BlockManagerId(1, algo-2, 34019, None)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:06,015 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:06,178 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.2.0/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:06,179 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.2.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:06,183 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@586f8ece{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:06,183 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15c985e4{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:06,183 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@385e6fd3{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:06,184 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ffbd15b{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:06,184 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6226f4d2{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:06,494 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:07,359 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:07,913 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:07,916 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, sex#0)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:07,917 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string>\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:07,923 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,286 INFO codegen.CodeGenerator: Code generated in 176.424483 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,344 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 444.5 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,393 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.5 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,395 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.202.182:43709 (size: 41.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,399 INFO spark.SparkContext: Created broadcast 0 from rdd at StringIndexer.scala:111\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,430 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,564 INFO spark.SparkContext: Starting job: countByValue at StringIndexer.scala:113\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,665 INFO scheduler.DAGScheduler: Registering RDD 6 (countByValue at StringIndexer.scala:113)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,666 INFO scheduler.DAGScheduler: Got job 0 (countByValue at StringIndexer.scala:113) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,667 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (countByValue at StringIndexer.scala:113)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,667 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,668 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,671 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[6] at countByValue at StringIndexer.scala:113), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,733 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.5 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,735 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,735 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.202.182:43709 (size: 8.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,736 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,747 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[6] at countByValue at StringIndexer.scala:113) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,748 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,768 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 5357 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:08,965 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:34019 (size: 8.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:09,725 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:34019 (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,536 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2774 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,538 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,542 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (countByValue at StringIndexer.scala:113) finished in 2.781 s\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,542 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,542 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,543 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,543 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,545 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[7] at countByValue at StringIndexer.scala:113), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,551 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,552 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1963.0 B, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,553 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.202.182:43709 (size: 1963.0 B, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,553 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,555 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[7] at countByValue at StringIndexer.scala:113) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,555 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,559 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-2, executor 1, partition 0, NODE_LOCAL, 4632 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,597 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:34019 (size: 1963.0 B, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,615 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.230.110:53098\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,618 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 137 bytes\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,665 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 108 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,665 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,666 INFO scheduler.DAGScheduler: ResultStage 1 (countByValue at StringIndexer.scala:113) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:11,670 INFO scheduler.DAGScheduler: Job 0 finished: countByValue at StringIndexer.scala:113, took 3.106046 s\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,083 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,083 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,084 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string, length: double, diameter: double, height: double, whole_weight: double ... 7 more fields>\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,084 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,244 INFO codegen.CodeGenerator: Code generated in 125.126087 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,261 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 444.5 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,273 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 41.5 KB, free 365.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,273 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.202.182:43709 (size: 41.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,274 INFO spark.SparkContext: Created broadcast 3 from javaToPython at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,274 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,433 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:12,433 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,046 INFO spark.SparkContext: Starting job: saveAsTextFile at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,047 INFO scheduler.DAGScheduler: Got job 1 (saveAsTextFile at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,047 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (saveAsTextFile at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,047 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,047 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,048 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,072 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 174.2 KB, free 365.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,074 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 64.1 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,075 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.202.182:43709 (size: 64.1 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,076 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,076 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,077 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,078 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-2, executor 1, partition 0, PROCESS_LOCAL, 5368 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,090 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:34019 (size: 64.1 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:13,920 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:34019 (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,369 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 3292 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,371 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,372 INFO scheduler.DAGScheduler: ResultStage 2 (saveAsTextFile at NativeMethodAccessorImpl.java:0) finished in 3.293 s\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,372 INFO scheduler.DAGScheduler: Job 1 finished: saveAsTextFile at NativeMethodAccessorImpl.java:0, took 3.325466 s\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,899 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,915 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.202.182:43709 in memory (size: 64.1 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,916 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:34019 in memory (size: 64.1 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,921 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.202.182:43709 in memory (size: 1963.0 B, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,922 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:34019 in memory (size: 1963.0 B, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,924 INFO spark.ContextCleaner: Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,924 INFO spark.ContextCleaner: Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,926 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,926 INFO spark.ContextCleaner: Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,929 INFO spark.ContextCleaner: Cleaned shuffle 0\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,930 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.202.182:43709 in memory (size: 41.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,932 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:34019 in memory (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,934 INFO spark.ContextCleaner: Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,935 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.202.182:43709 in memory (size: 8.5 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:16,936 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:34019 in memory (size: 8.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,190 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,190 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,191 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string, length: double, diameter: double, height: double, whole_weight: double ... 7 more fields>\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,191 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,286 INFO codegen.CodeGenerator: Code generated in 80.099081 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,298 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 444.5 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,310 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 41.5 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,310 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.202.182:43709 (size: 41.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,311 INFO spark.SparkContext: Created broadcast 5 from javaToPython at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,312 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,414 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,414 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,863 INFO spark.SparkContext: Starting job: saveAsTextFile at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,864 INFO scheduler.DAGScheduler: Got job 2 (saveAsTextFile at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,864 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,864 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,865 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,865 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,882 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 174.2 KB, free 365.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,884 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 64.1 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,884 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.202.182:43709 (size: 64.1 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,885 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,885 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[21] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,885 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,886 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 5368 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:17,902 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:34019 (size: 64.1 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:18,105 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:34019 (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:19,765 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1879 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:19,765 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:19,766 INFO scheduler.DAGScheduler: ResultStage 3 (saveAsTextFile at NativeMethodAccessorImpl.java:0) finished in 1.880 s\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:19,766 INFO scheduler.DAGScheduler: Job 2 finished: saveAsTextFile at NativeMethodAccessorImpl.java:0, took 1.903574 s\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:20,889 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:20,889 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:20,890 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string, length: double, diameter: double, height: double, whole_weight: double ... 7 more fields>\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:20,890 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:20,986 INFO codegen.CodeGenerator: Code generated in 55.436209 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,003 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 444.5 KB, free 364.7 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,014 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 41.5 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,015 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.202.182:43709 (size: 41.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,016 INFO spark.SparkContext: Created broadcast 7 from sparkToMleapDataShape at VectorAssemblerOp.scala:26\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,017 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,027 INFO spark.SparkContext: Starting job: sparkToMleapDataShape at VectorAssemblerOp.scala:26\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,028 INFO scheduler.DAGScheduler: Got job 3 (sparkToMleapDataShape at VectorAssemblerOp.scala:26) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,028 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (sparkToMleapDataShape at VectorAssemblerOp.scala:26)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,028 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,028 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,028 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[24] at sparkToMleapDataShape at VectorAssemblerOp.scala:26), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,032 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 56.4 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,034 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.7 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,034 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.202.182:43709 (size: 20.7 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,035 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,035 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[24] at sparkToMleapDataShape at VectorAssemblerOp.scala:26) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,036 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,037 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, algo-2, executor 1, partition 0, PROCESS_LOCAL, 5368 bytes)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,051 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:34019 (size: 20.7 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,124 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:34019 (size: 41.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,276 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 240 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,277 INFO scheduler.DAGScheduler: ResultStage 4 (sparkToMleapDataShape at VectorAssemblerOp.scala:26) finished in 0.241 s\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,278 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,278 INFO scheduler.DAGScheduler: Job 3 finished: sparkToMleapDataShape at VectorAssemblerOp.scala:26, took 0.251225 s\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,301 INFO codegen.CodeGenerator: Code generated in 9.853794 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,556 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,569 INFO server.AbstractConnector: Stopped Spark@70ed614f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,571 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,571 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,571 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,571 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,574 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.202.182:43709 in memory (size: 20.7 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,575 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:34019 in memory (size: 20.7 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,575 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.202.182:4040\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,580 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,589 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,590 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,592 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,594 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,599 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,604 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,605 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,605 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,607 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,615 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,615 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,616 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3661bfcc-5d8c-4484-921c-8b7b9142af23\u001b[0m\n",
      "\u001b[34m2020-06-12 20:03:21,616 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3661bfcc-5d8c-4484-921c-8b7b9142af23/pyspark-408ef6cb-9b0d-4b22-b4ae-de4ab7f8f227\u001b[0m\n",
      "\u001b[35m2020-06-12 20:03:23\u001b[0m\n",
      "\u001b[35mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mReceived end of job signal, exiting...\u001b[0m\n",
      "\u001b[34mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput\n",
    "spark_processor = ScriptProcessor(base_job_name='spark-preprocessor',\n",
    "                                  image_uri=spark_repository_uri,\n",
    "                                  command=['/opt/program/submit'],\n",
    "                                  role=role,\n",
    "                                  instance_count=2,\n",
    "                                  instance_type='ml.r5.xlarge',\n",
    "                                  max_runtime_in_seconds=1200,\n",
    "                                  env={'mode': 'python'})\n",
    "\n",
    "spark_processor.run(code='preprocess.py',\n",
    "                    arguments=['s3_input_bucket', bucket,\n",
    "                               's3_input_key_prefix', input_prefix,\n",
    "                               's3_output_bucket', bucket,\n",
    "                               's3_output_key_prefix', input_preprocessed_prefix,\n",
    "                               's3_model_bucket', bucket,\n",
    "                               's3_mleap_model_prefix', mleap_model_prefix],\n",
    "                    logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://sagemaker-us-east-1-885332847160/sagemaker/spark-preprocess-demo/2020-06-12-19-58-14/input/preprocessed/abalone/train/\n",
      "5.0,0.0,0.0,0.275,0.195,0.07,0.08,0.031,0.0215,0.025\n",
      "6.0,0.0,0.0,0.29,0.21,0.075,0.275,0.113,0.0675,0.035\n",
      "7.0,0.0,0.0,0.305,0.225,0.07,0.1485,0.0585,0.0335,0.045\n",
      "7.0,0.0,0.0,0.305,0.23,0.08,0.156,0.0675,0.0345,0.048\n",
      "7.0,0.0,0.0,0.325,0.26,0.09,0.1915,0.085,0.036,0.062\n"
     ]
    }
   ],
   "source": [
    "print('Top 5 rows from s3://{}/{}/train/'.format(bucket, input_preprocessed_prefix))\n",
    "!aws s3 cp --quiet s3://$bucket/$input_preprocessed_prefix/train/part-00000 - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:0.90-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sagemaker_session.boto_region_name, 'xgboost', repo_version=\"0.90-1\")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, input_preprocessed_prefix, 'train/part')\n",
    "s3_validation_data = 's3://{}/{}/{}'.format(bucket, input_preprocessed_prefix, 'validation/part')\n",
    "s3_output_location = 's3://{}/{}/{}'.format(bucket, prefix, 'xgboost_model')\n",
    "\n",
    "xgb_model = sagemaker.estimator.Estimator(training_image,\n",
    "                                          role, \n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m4.xlarge',\n",
    "                                          train_volume_size = 20,\n",
    "                                          train_max_run = 3600,\n",
    "                                          input_mode= 'File',\n",
    "                                          output_path=s3_output_location,\n",
    "                                          sagemaker_session=sagemaker_session)\n",
    "\n",
    "xgb_model.set_hyperparameters(objective = \"reg:linear\",\n",
    "                              eta = .2,\n",
    "                              gamma = 4,\n",
    "                              max_depth = 5,\n",
    "                              num_round = 10,\n",
    "                              subsample = 0.7,\n",
    "                              silent = 0,\n",
    "                              min_child_weight = 6)\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/csv', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/csv', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 20:05:06 Starting - Starting the training job...\n",
      "2020-06-12 20:05:09 Starting - Launching requested ML instances.........\n",
      "2020-06-12 20:06:53 Starting - Preparing the instances for training......\n",
      "2020-06-12 20:08:04 Downloading - Downloading input data...\n",
      "2020-06-12 20:08:29 Training - Downloading the training image...\n",
      "2020-06-12 20:09:05 Uploading - Uploading generated training model\n",
      "2020-06-12 20:09:05 Completed - Training job completed\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value reg:linear to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[20:08:55] 3320x9 matrix with 29880 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[20:08:55] 857x9 matrix with 7713 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 3320 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 857 rows\u001b[0m\n",
      "\u001b[34m[20:08:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:8.08026#011validation-rmse:8.20142\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:6.60688#011validation-rmse:6.72072\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:5.45351#011validation-rmse:5.56356\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:4.54415#011validation-rmse:4.66501\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:3.84212#011validation-rmse:3.96408\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:3.31659#011validation-rmse:3.46107\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:2.91519#011validation-rmse:3.08405\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:2.61875#011validation-rmse:2.8029\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:2.40996#011validation-rmse:2.60705\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:2.26116#011validation-rmse:2.4626\u001b[0m\n",
      "Training seconds: 61\n",
      "Billable seconds: 61\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"name\": \"sex\", \"type\": \"string\"}, {\"name\": \"length\", \"type\": \"double\"}, {\"name\": \"diameter\", \"type\": \"double\"}, {\"name\": \"height\", \"type\": \"double\"}, {\"name\": \"whole_weight\", \"type\": \"double\"}, {\"name\": \"shucked_weight\", \"type\": \"double\"}, {\"name\": \"viscera_weight\", \"type\": \"double\"}, {\"name\": \"shell_weight\", \"type\": \"double\"}], \"output\": {\"name\": \"features\", \"type\": \"double\", \"struct\": \"vector\"}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "schema = {\n",
    "    \"input\": [\n",
    "        {\n",
    "            \"name\": \"sex\",\n",
    "            \"type\": \"string\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"length\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"diameter\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"height\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"whole_weight\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"shucked_weight\",\n",
    "            \"type\": \"double\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"viscera_weight\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "        {\n",
    "            \"name\": \"shell_weight\",\n",
    "            \"type\": \"double\"\n",
    "        }, \n",
    "    ],\n",
    "    \"output\": \n",
    "        {\n",
    "            \"name\": \"features\",\n",
    "            \"type\": \"double\",\n",
    "            \"struct\": \"vector\"\n",
    "        }\n",
    "}\n",
    "schema_json = json.dumps(schema)\n",
    "print(schema_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.sparkml.model import SparkMLModel\n",
    "\n",
    "sparkml_data = 's3://{}/{}/{}'.format(bucket, mleap_model_prefix, 'model.tar.gz')\n",
    "# passing the schema defined above by using an environment variable that sagemaker-sparkml-serving understands\n",
    "sparkml_model = SparkMLModel(model_data=sparkml_data, env={'SAGEMAKER_SPARKML_SCHEMA' : schema_json})\n",
    "xgb_model = Model(model_data=xgb_model.model_data, image=training_image)\n",
    "\n",
    "model_name = 'inference-pipeline-' + timestamp_prefix\n",
    "sm_model = PipelineModel(name=model_name, role=role, models=[sparkml_model, xgb_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'inference-pipeline-ep-' + timestamp_prefix\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'11.162177085876465'\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "payload = \"F,0.515,0.425,0.14,0.766,0.304,0.1725,0.255\"\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, serializer=csv_serializer,\n",
    "                                content_type=CONTENT_TYPE_CSV, accept=CONTENT_TYPE_CSV)\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'11.162177085876465'\n"
     ]
    }
   ],
   "source": [
    "payload = {\"data\": [\"F\",0.515,0.425,0.14,0.766,0.304,0.1725,0.255]}\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, sagemaker_session=sagemaker_session, serializer=json_serializer,\n",
    "                                content_type=CONTENT_TYPE_JSON, accept=CONTENT_TYPE_CSV)\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-12 20:16:51--  https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/batch_input_abalone.csv\n",
      "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.248.8\n",
      "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.248.8|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 654 [text/csv]\n",
      "Saving to: ‘batch_input_abalone.csv.1’\n",
      "\n",
      "batch_input_abalone 100%[===================>]     654  --.-KB/s    in 0s      \n",
      "\n",
      "2020-06-12 20:16:51 (17.6 MB/s) - ‘batch_input_abalone.csv.1’ saved [654/654]\n",
      "\n",
      "\n",
      "\n",
      "Showing first five lines\n",
      "\n",
      "M,0.455,0.365,0.095,0.514,0.2245,0.101,0.15\n",
      "M,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07\n",
      "F,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21\n",
      "M,0.44,0.365,0.125,0.516,0.2155,0.114,0.155\n",
      "I,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055\n",
      "\n",
      "\n",
      "As we can see, it is identical to the training file apart from the label being absent here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3-us-west-2.amazonaws.com/sparkml-mleap/data/batch_input_abalone.csv\n",
    "!printf \"\\n\\nShowing first five lines\\n\\n\"    \n",
    "!head -n 5 batch_input_abalone.csv \n",
    "!printf \"\\n\\nAs we can see, it is identical to the training file apart from the label being absent here.\\n\\n\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_loc = sagemaker_session.upload_data(path='batch_input_abalone.csv', bucket=bucket, key_prefix=prefix+'/batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[34m  .   ____          _            __ _ _\n",
      " /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\u001b[0m\n",
      "\u001b[35m  .   ____          _            __ _ _\n",
      " /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\u001b[0m\n",
      "\u001b[34m( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n",
      " \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n",
      "  '  |____| .__|_| |_|_| |_\\__, | / / / /\n",
      " =========|_|==============|___/=/_/_/_/\n",
      " :: Spring Boot ::                  (v2.2)\n",
      "\u001b[0m\n",
      "\u001b[34m2020-06-12 20:20:58.238  INFO 7 --- [           main] com.amazonaws.sagemaker.App              : Starting App v2.2 on aa16cce3a6af with PID 7 (/usr/local/lib/sparkml-serving-2.2.jar started by root in /sagemaker-sparkml-model-server)\u001b[0m\n",
      "\u001b[34m2020-06-12 20:20:58.254  INFO 7 --- [           main] com.amazonaws.sagemaker.App              : No active profile set, falling back to default profiles: default\u001b[0m\n",
      "\u001b[35m( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\n",
      " \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n",
      "  '  |____| .__|_| |_|_| |_\\__, | / / / /\n",
      " =========|_|==============|___/=/_/_/_/\n",
      " :: Spring Boot ::                  (v2.2)\n",
      "\u001b[0m\n",
      "\u001b[35m2020-06-12 20:20:58.238  INFO 7 --- [           main] com.amazonaws.sagemaker.App              : Starting App v2.2 on aa16cce3a6af with PID 7 (/usr/local/lib/sparkml-serving-2.2.jar started by root in /sagemaker-sparkml-model-server)\u001b[0m\n",
      "\u001b[35m2020-06-12 20:20:58.254  INFO 7 --- [           main] com.amazonaws.sagemaker.App              : No active profile set, falling back to default profiles: default\u001b[0m\n",
      "\u001b[32m[2020-06-12 20:20:57 +0000] [15] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[32m[2020-06-12 20:20:57 +0000] [15] [INFO] Listening at: unix:/tmp/gunicorn.sock (15)\u001b[0m\n",
      "\u001b[32m[2020-06-12 20:20:57 +0000] [15] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[32m[2020-06-12 20:20:57 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[32m[2020-06-12 20:20:57 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[32m[2020-06-12 20:20:57 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[32m[2020-06-12 20:20:57 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.078  INFO 7 --- [           main] org.eclipse.jetty.util.log               : Logging initialized @5691ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.213  INFO 7 --- [           main] o.s.b.w.e.j.JettyServletWebServerFactory : Server initialized with port: 8080\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.222  INFO 7 --- [           main] org.eclipse.jetty.server.Server          : jetty-9.4.z-SNAPSHOT; built: 2018-08-30T13:59:14.071Z; git: 27208684755d94a92186989f695db2d7b21ebc51; jvm 1.8.0_181-8u181-b13-2~deb9u1-b13\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.377  INFO 7 --- [           main] org.eclipse.jetty.server.session         : DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.078  INFO 7 --- [           main] org.eclipse.jetty.util.log               : Logging initialized @5691ms to org.eclipse.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.213  INFO 7 --- [           main] o.s.b.w.e.j.JettyServletWebServerFactory : Server initialized with port: 8080\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.222  INFO 7 --- [           main] org.eclipse.jetty.server.Server          : jetty-9.4.z-SNAPSHOT; built: 2018-08-30T13:59:14.071Z; git: 27208684755d94a92186989f695db2d7b21ebc51; jvm 1.8.0_181-8u181-b13-2~deb9u1-b13\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.377  INFO 7 --- [           main] org.eclipse.jetty.server.session         : DefaultSessionIdManager workerName=node0\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.378  INFO 7 --- [           main] org.eclipse.jetty.server.session         : No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.379  INFO 7 --- [           main] org.eclipse.jetty.server.session         : node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.387  INFO 7 --- [           main] o.e.j.s.h.ContextHandler.application     : Initializing Spring embedded WebApplicationContext\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.387  INFO 7 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 3895 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.419  INFO 7 --- [           main] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.422  INFO 7 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.422  INFO 7 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.422  INFO 7 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'formContentFilter' to: [/*]\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.422  INFO 7 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.428  INFO 7 --- [           main] o.e.jetty.server.handler.ContextHandler  : Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@aa22f1c{application,/,[file:///tmp/jetty-docbase.4668639577432682607.8080/],AVAILABLE}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:02.429  INFO 7 --- [           main] org.eclipse.jetty.server.Server          : Started @6043ms\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.378  INFO 7 --- [           main] org.eclipse.jetty.server.session         : No SessionScavenger set, using defaults\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.379  INFO 7 --- [           main] org.eclipse.jetty.server.session         : node0 Scavenging every 600000ms\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.387  INFO 7 --- [           main] o.e.j.s.h.ContextHandler.application     : Initializing Spring embedded WebApplicationContext\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.387  INFO 7 --- [           main] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 3895 ms\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.419  INFO 7 --- [           main] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.422  INFO 7 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.422  INFO 7 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.422  INFO 7 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'formContentFilter' to: [/*]\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.422  INFO 7 --- [           main] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.428  INFO 7 --- [           main] o.e.jetty.server.handler.ContextHandler  : Started o.s.b.w.e.j.JettyEmbeddedWebAppContext@aa22f1c{application,/,[file:///tmp/jetty-docbase.4668639577432682607.8080/],AVAILABLE}\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:02.429  INFO 7 --- [           main] org.eclipse.jetty.server.Server          : Started @6043ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:04.584  INFO 7 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:04.974  INFO 7 --- [           main] o.e.j.s.h.ContextHandler.application     : Initializing Spring DispatcherServlet 'dispatcherServlet'\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:04.974  INFO 7 --- [           main] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:04.981  INFO 7 --- [           main] o.s.web.servlet.DispatcherServlet        : Completed initialization in 7 ms\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:05.012  INFO 7 --- [           main] o.e.jetty.server.AbstractConnector       : Started ServerConnector@5e944453{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:04.584  INFO 7 --- [           main] o.s.s.concurrent.ThreadPoolTaskExecutor  : Initializing ExecutorService 'applicationTaskExecutor'\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:04.974  INFO 7 --- [           main] o.e.j.s.h.ContextHandler.application     : Initializing Spring DispatcherServlet 'dispatcherServlet'\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:04.974  INFO 7 --- [           main] o.s.web.servlet.DispatcherServlet        : Initializing Servlet 'dispatcherServlet'\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:04.981  INFO 7 --- [           main] o.s.web.servlet.DispatcherServlet        : Completed initialization in 7 ms\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:05.012  INFO 7 --- [           main] o.e.jetty.server.AbstractConnector       : Started ServerConnector@5e944453{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:05.016  INFO 7 --- [           main] o.s.b.web.embedded.jetty.JettyWebServer  : Jetty started on port(s) 8080 (http/1.1) with context path '/'\u001b[0m\n",
      "\u001b[34m2020-06-12 20:21:05.021  INFO 7 --- [           main] com.amazonaws.sagemaker.App              : Started App in 7.867 seconds (JVM running for 8.635)\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:05.016  INFO 7 --- [           main] o.s.b.web.embedded.jetty.JettyWebServer  : Jetty started on port(s) 8080 (http/1.1) with context path '/'\u001b[0m\n",
      "\u001b[35m2020-06-12 20:21:05.021  INFO 7 --- [           main] com.amazonaws.sagemaker.App              : Started App in 7.867 seconds (JVM running for 8.635)\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:07:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:07 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:07:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:07 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[20:21:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[20:21:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[20:21:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[20:21:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 19 \"-\" \"Go-http-client/1.1\"\n",
      "\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 19 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 16 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 19 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 19 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 19 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m[2020-06-12:20:21:08:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m169.254.255.130 - - [12/Jun/2020:20:21:08 +0000] \"POST /invocations HTTP/1.1\" 200 18 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      "\u001b[36m2020-06-12T20:21:07.794:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=5, BatchStrategy=SINGLE_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_data_path = 's3://{}/{}/{}'.format(bucket, prefix + '/batch', 'batch_input_abalone.csv')\n",
    "output_data_path = 's3://{}/{}'.format(bucket, prefix + '/batch_output/abalone')\n",
    "job_name = 'serial-inference-batch-' + timestamp_prefix\n",
    "transformer = sagemaker.transformer.Transformer(\n",
    "    # This was the model created using PipelineModel and it contains feature processing and XGBoost\n",
    "    model_name = model_name,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m4.xlarge',\n",
    "    strategy = 'SingleRecord',\n",
    "    assemble_with = 'Line',\n",
    "    output_path = output_data_path,\n",
    "    base_transform_job_name='serial-inference-batch',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    accept = CONTENT_TYPE_CSV\n",
    ")\n",
    "transformer.transform(data = input_data_path,\n",
    "                      job_name = job_name,\n",
    "                      content_type = CONTENT_TYPE_CSV, \n",
    "                      split_type = 'Line')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.722335815429688\r\n",
      "7.243444442749023\r\n",
      "9.691205978393555\r\n",
      "9.056212425231934\r\n",
      "5.777616500854492\r\n",
      "6.850498199462891\r\n",
      "12.855364799499512\r\n",
      "10.699277877807617\r\n",
      "9.3707914352417\r\n",
      "11.894083976745605\r\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "KEY = prefix + '/batch_output/abalone/batch_input_abalone.csv.out'\n",
    "s3.Bucket(bucket).download_file(KEY, 'batch_output_abalone.csv')\n",
    "\n",
    "!head batch_output_abalone.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
